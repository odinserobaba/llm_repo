{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab: Merge LoRA-адаптера в базовую модель\n",
    "\n",
    "**Цель:** объединить веса адаптера с базовой моделью. Получится один файл — без PEFT, готов к деплою.\n",
    "\n",
    "> После merge переключать адаптеры нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Установка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка и merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "ADAPTER_PATH = \"outputs_finetome/final\"  # путь к адаптеру (или Drive)\n",
    "OUTPUT_DIR = \"mistral-7b-finetome-merged\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    ADAPTER_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Merge: LoRA веса вливаются в base\n",
    "merged = model.merge_and_unload()\n",
    "print(\"Merge выполнен.\")\n",
    "\n",
    "# Сохранение\n",
    "merged.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Модель сохранена в {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Сохранение в Google Drive (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "# merged.save_pretrained(\"/content/drive/MyDrive/models/mistral-7b-merged\")\n",
    "# tokenizer.save_pretrained(\"/content/drive/MyDrive/models/mistral-7b-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Загрузка merged-модели (без PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged модель — обычный AutoModelForCausalLM, peft не нужен\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
