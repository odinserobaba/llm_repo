# ДЗ 8: Fine-tuning + LangChain Tools

## Общая цель

Научить большую языковую модель (LLM) двум вещам:

1. **Лучше следовать инструкциям** — через fine-tuning на качественных примерах.
2. **Расширить возможности** — подключить внешние инструменты (форматирование, шаблоны, проверки), которые модель сама «не умеет», но может вызывать.

Базовая Mistral-7B знает языки и факты, но не специализирована под формат «инструкция → чёткий ответ». Fine-tuning + tools превращают её в **инструктивного помощника**, который и отвечает по делу, и делегирует части задач инструментам.

---

## Выбранный трек: C — Инструктивный помощник

- **Датасет:** [mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) — 100k примеров диалогов формата «вопрос → развёрнутый ответ».
- **Модели:** TinyLlama-1.1B (быстрый показ ~1 ч, публичная) или Mistral-7B (качественнее ~5 ч), 4-bit + LoRA.
- **Tools:** `text_formatter`, `template_generator`, `content_validator`.

---

## Структура файлов

| Файл | Описание |
|------|----------|
| `peft_tools_hw.ipynb` | Основной ноутбук: fine-tuning, tools, интеграция |
| `prompt_cognitive_designer.md` | Системный промпт «когнитивный дизайнер» (опционально) |
| `adapters_colab/` | **Работа с адаптерами** — загрузка, переключение, merge, роутер (Colab) |

---

## Что такое Fine-tuning

**Fine-tuning** (дообучение) — это процесс, когда уже обученная нейросеть дополнительно тренируется на специфичных примерах под конкретную задачу. Мы не учим модель с нуля, а «подгоняем» её веса под новые данные.

### Почему не обучать с нуля?

| Подход | Время | Ресурсы | Данные |
|-------|-------|---------|--------|
| Обучение с нуля | Месяцы | Десятки/сотни GPU | Триллионы токенов |
| Fine-tuning | Часы | 1 GPU (Colab) | Тысячи примеров |

Базовая модель (Mistral, LLaMA и т.п.) уже «знает» язык, факты, рассуждения — всё это заложено в 7 млрд параметров. Fine-tuning лишь корректирует поведение под наш формат и стиль.

### Аналогия

Представь переводчика, который учил английский годами. Вместо того чтобы учить его заново, мы даём ему **1000 примеров**: «когда тебя спрашивают вот так — отвечай вот так». Он сохраняет все прежние знания, но начинает чаще следовать нашему шаблону.

### Что именно меняется при fine-tuning?

В модели миллиарды весов — чисел, которые определяют, какой следующий токен модель «предполагает». При обучении мы:

1. **Подаём пару** (Instruction, Response).
2. **Модель генерирует** Response по Instruction.
3. **Сравниваем** с эталонным Response.
4. **Обновляем веса** так, чтобы ошибка уменьшалась (градиентный спуск).

Чем больше похожих примеров, тем устойчивее модель следует нужному формату.

### Full fine-tuning vs LoRA

| Метод | Обучаемые веса | Память | Результат |
|-------|----------------|--------|-----------|
| Full fine-tuning | Все 7 млрд | ~80+ GB GPU | Максимальная гибкость, но дорого |
| LoRA | ~4 млн (0.05%) | ~16 GB | Почти такой же эффект при 20× экономии |

**LoRA** (Low-Rank Adaptation) не трогает исходные веса. Внутрь attention-слоёв «встраиваются» малые матрицы A и B; при инференсе их результат добавляется к выходу. Обучаем только A и B — получаем компактный адаптер ~50 MB, который можно накладывать на базовую модель.

### SFT (Supervised Fine-Tuning)

**SFT** — это fine-tuning, когда у каждого примера есть **правильный ответ**. Модель учится минимизировать cross-entropy loss: «насколько моя генерация отличается от эталона». Это прямой способ научить модель формату и стилю — без reinforcement learning или сложных reward-моделей.

---

## Подробное описание этапов

### Этап 1: Fine-tuning

**Что делаем.** Грузим Mistral-7B в 4-bit, обучаем на FineTome-100k только малые LoRA-адаптеры (0.1% параметров), сохраняем результат.

**Зачем.** Базовая модель обучена предсказывать следующий токен на общих текстах. Наш fine-tuning учит её **формату инструкций** и **стилю развёрнутых ответов**. Модель начинает чаще следовать шаблону «Instruction → Response» и давать структурированные ответы.

**Как устроен шаг:**

1. **4-bit квантизация** — уменьшает объём памяти. Mistral-7B целиком не влезет в Colab T4 (~16 GB). В 4-bit помещается и обучение становится возможным.
2. **LoRA** — обучаем только малые матрицы, «привязанные» к attention-слоям. Остальные веса заморожены. Быстрее, дешевле, адаптер ~50 MB.
3. **FineTome-100k** — диалоги human/gpt в стиле «объясни рекурсию», «напиши функцию факториала». Преобразуем в текст:
   ```
   ### Instruction:
   Explain recursion in programming.
   
   ### Response:
   Recursion is when a function calls itself...
   ```
4. **SFTTrainer** — supervised fine-tuning: модель учится генерировать Response по Instruction.

**Небольшой пример до/после:**

```
До:  "рекурсия это когда функция вызывает саму себя" (коротко, сухо)
После: "Recursion is a technique where a function calls itself... Here's an example in Python: ..."
```

---

### Этап 2: LangChain Tools

**Что делаем.** Пишем 2–3 функции с декоратором `@tool` и валидацией входов.

**Зачем.** LLM умеет только генерировать текст. Реальные задачи часто требуют:
- форматировать длинный текст по ширине;
- подставить данные в шаблон;
- проверить, что ответ валиден по правилам.

Tools дают модели «руки»: вместо «придумываю на ходу» она может вызвать проверенную функцию и вернуть результат.

**Каждый tool и его цель:**

| Tool | Цель | Пример |
|------|------|--------|
| `text_formatter` | Перенос текста по ширине, отступы | Длинная строка → читаемый параграф с отступом 2 |
| `template_generator` | Выдать готовый шаблон по типу | `template_type="prompt"` → шаблон Instruction/Input/Response |
| `content_validator` | Проверить текст по правилам | Есть ли заголовки, не пустой ли, похож ли на email |

**Пример вызова:**

```python
text_formatter.invoke({"text": "Очень длинная строка...", "width": 40, "indent": 2})
# → отформатированный текст с переносами
```

---

### Этап 3: Интеграция (модель + tools)

**Что делаем.** Загружаем fine-tuned модель, оборачиваем в `HuggingFacePipeline`, подключаем tools, показываем сценарии:
- генерация ответа по инструкции (прямой вызов модели);
- ручной вызов tools (Mistral без нативного tool calling).

**Зачем.** Показать полный цикл: **обученная модель + инструменты** работают вместе. В идеале модель сама решает, когда вызвать tool (для этого нужна поддержка tool calling, как у GPT-4). Mistral её не имеет, поэтому демонстрируем:
1. Ответы fine-tuned модели — улучшение по сравнению с базовой.
2. Прямые вызовы tools — как они расширяют возможности системы.

**Пример сценария:**

```
Пользователь: "Отформатируй этот текст по 40 символов"
→ Вызываем text_formatter с текстом
→ Модель (или мы) подготавливает итоговый ответ на основе результата tool
```

---

### Этап 4 (опционально): системный промпт «когнитивный дизайнер»

**Что делаем.** Добавляем в начало промпта инструкции из `prompt_cognitive_designer.md`: объяснять через аналогии, якоря, без «как известно», с контрастом ошибки.

**Зачем.** Fine-tuning учит формат, системный промпт задаёт **стиль** объяснений. Модель начинает строить ответы как когнитивный дизайнер: по схеме «якорь → суть → механика → прорыв», что улучшает запоминание и понимание.

---

## Зависимости

- Уроки 01–07 из `07_finetuning_lora/` — базовые понятия LoRA, SFTTrainer, деплой.
- `03_module_agents/01_custom_tools.ipynb` — создание tools с `args_schema` и валидацией.

---

## Порядок работы

1. Запустить `peft_tools_hw.ipynb` в Colab (GPU).
2. Выполнить ячейки по порядку: установка → fine-tuning → tools → интеграция.
3. После обучения проверить качество ответов и работу tools.
4. При желании подключить системный промпт «когнитивный дизайнер».

---

## Работа с несколькими адаптерами

Если обучено несколько специализаций (JSON, кодинг, техподдержка) — см. папку **`adapters_colab/`**:

- `01_load_switch_adapters.ipynb` — загрузка base и переключение адаптеров
- `02_merge_adapter.ipynb` — merge адаптера в base для деплоя
- `03_router_demo.ipynb` — роутер по типу запроса

Подробности в `adapters_colab/README.md`.
