# ДЗ 8: Fine-tuning + LangChain Tools

```
┌─────────────────────────────────────────────────────────────┐
│  ДЗ 8 как инструктивный помощник: модель с «руками»         │
│  (tools) — отвечает развёрнуто и вызывает инструменты       │
├────────────┬────────────┬────────────┬──────────────────────┤
│ Якорь     │ Механика   │ Прорыв     │ Применение           │
│ 3 сек     │ 20 сек     │ ✨ 5 сек   │ 10 сек               │
│ Переводчик│ FT + Tools │ Делегирует │ Когнитивный дизайнер │
│ + примеры │            │ задачу     │ системный промпт     │
└────────────┴────────────┴────────────┴──────────────────────┘
```

---

## 🎯 Якорь + эмоциональный мостик

**🎯 ЯКОРЬ:** Переводчик учил английский годами. Мы даём 1000 примеров «вопрос → развёрнутый ответ» и подключаем «руки»: форматирование, шаблоны, проверки. Модель знает, когда вызвать инструмент.

💡 **Эмпатия:** «Звучит как два в одном? Не переживай — сначала освоишь один инструмент (форматирование), потом добавишь остальные. Шаг за шагом станет проще, чем настроить принтер.»

---

## 📖 Термины и понятия

| Термин | Что это | Метафора |
|--------|---------|----------|
| **Mistral-7B** | Открытая LLM на 7 млрд параметров. В 4-bit помещается в Colab T4. | 🧠 Умный ассистент в кармане |
| **TinyLlama-1.1B** | Малая модель 1.1B. Быстрая для демо (~2–4 GB VRAM). | 🐣 Быстрый старт без ожидания |
| **FineTome-100k** | 100k диалогов «instruction → response». Для SFT. | 📚 Учебник с готовыми ответами |
| **Tools (LangChain)** | Функции с `@tool`, которые модель может вызвать. | ✋ Руки у робота |
| **4-bit + LoRA** | Квантизация 4-bit + LoRA (обучаем 0.05%). | ✂️ Подгонка костюма, не пошив |
| **OOM / VRAM** | Не хватает памяти видеокарты. T4=16 GB. | ⚠️ Сигнал перегрузки |

---

## 📐 Радиальная карта знаний

```
                         ┌──────────────────┐
                         │   ДЗ 8            │ ← ЦЕНТР
                         │   Модель + Руки   │
                         └────────┬─────────┘
              ┌───────────────────┼───────────────────┐
              ↓                   ↓                    ↓
       ┌────────────┐     ┌────────────┐     ┌────────────┐
       │  Учим     │     │  Даём     │     │  Связываем │
       │  формат   │     │  руки     │     │  вместе    │
       └─────┬──────┘     └─────┬──────┘     └─────┬──────┘
             ↓                   ↓                   ↓
       • FineTome-100k     • text_formatter    • вызов инструмента
       • LoRA 4-bit        • template_gen     • развёрнутый ответ
       • адаптер ~50 МБ    • content_validator
```

---

## 🔷 Прогрессивная схема (3 фазы)

#### Фаза 1: Учим формат (Fine-tuning)
```
┌──────────────┐     ┌──────────────┐
│  Вопрос      │ ─→  │  Базовая     │
│  «Что такое  │     │  модель      │ ← сухо, коротко
│  рекурсия?»  │     │  Mistral-7B  │
└──────────────┘     └──────┬───────┘
                           ↓
┌──────────────┐     ┌──────────────┐
│  FineTome-   │ ─→  │  Адаптер     │ ← ✨ учит развёрнуто отвечать
│  100k        │     │  LoRA 4-bit  │
└──────────────┘     └──────────────┘
```

#### Фаза 2: Даём руки (Tools)
```
┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│ text_        │     │ template_    │     │ content_    │
│ formatter    │     │ generator    │     │ validator   │
└──────────────┘     └──────────────┘     └──────────────┘
       │                    │                    │
       └────────────────────┴────────────────────┘
                          ↓
                  ✋ «Руки» для задач
```

#### Фаза 3: Прорыв ← ✨ ВОТ ЗДЕСЬ МАГИЯ!
```
┌───────────────────────────────────────────────┐
│ Пользователь: «Отформатируй текст по 40 симв.»│
└───────────────────────┬───────────────────────┘
                        ↓
          ┌───────────────────────────┐
          │ Модель распознаёт задачу  │
          └───────────────┬───────────┘
                          ↓
    ┌───────────────────────────────────────┐
    │ Вызывает → text_formatter(width=40)   │ ← ✨ МАГИЯ: делегирует задачу!
    └───────────────────────┬───────────────┘
                            ↓
          ┌───────────────────────────┐
          │ Возвращает отформатиров.  │
          │ текст ← идеально по ширине│
          └───────────────────────────┘
```

> ✨ **Пик:** Модель **не пытается сама** форматировать! Она **вызывает инструмент** — как человек просит помочь с задачей, которую сам не умеет.

---

## 📊 Таблица контрастов

| Что думают ❌ | Что на самом деле ✅ | Визуальная метафора |
|---------------|---------------------|---------------------|
| «FT = обучение с нуля» | Подгоняем 0.05% весов под формат | ✂️ Подгонка vs 🧵 Пошив |
| «Модель сама форматирует» | Вызывает инструмент `text_formatter` | 🤖 Робот с руками vs 🧠 Только мозг |
| «До FT = умная модель» | Базовая — сухо, без примеров | До: «рекурсия — вызов себя» / После: развёрнуто + пример |
| «Один адаптер на всё» | JSON, coding, support — сменные | 🔧 Сменные насадки |

---

## 💻 Мини-код с комментариями-стрелками

```python
# ← 1. Загружаем модель в 4-bit (экономим память)
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", load_in_4bit=True)
# ← 2. SFT на FineTome-100k
trainer = SFTTrainer(model=model, train_dataset=fine_tome_100k, peft_config=lora_config)
#        ↑
#        └─── ✨ МАГИЯ: модель учится отвечать развёрнуто!

trainer.train()
# ← 3. Инструмент «руки»
@tool
def text_formatter(text: str, width: int = 40) -> str:
    return textwrap.fill(text, width=width)
#        ↑
#        └─── ✨ МАГИЯ: модель вызывает эту функцию вместо самой форматировать!
```

---

## ✅ Чек-лист самопроверки

```
✅ Проверь себя за 15 секунд:
▫️ Могу объяснить: FT учит формат, tools — дают «руки»
▫️ Знаю три tools: formatter, template, validator
▫️ Вижу разницу до/после FT (сухо → развёрнуто с примерами)
▫️ Понимаю прорыв: модель делегирует задачу инструменту
▫️ Могу переключать адаптеры через adapters_colab

→ Если 4+ галочки — готов к практике.
```

---

## 🔍 Микро-проверка

**Вопрос:** Почему после fine-tuning ответы стали развёрнутыми?  
**Ответ:** Модель видела 100k примеров «вопрос → развёрнутый ответ» в FineTome-100k — скопировала стиль. Как ученик с решебником.

**Вопрос:** Почему модель вызывает `text_formatter`, а не форматирует сама?  
**Ответ:** Форматирование — механическая задача. Модель «знает», что для этого есть инструмент — как человек вызывает калькулятор для вычислений.

---

## ⚠️ Частые ошибки

| Ошибка | Почему | Решение |
|--------|--------|---------|
| OOM при обучении | Слишком большой batch_size для 16 GB VRAM | `batch_size=1` + `gradient_accumulation_steps=4` |
| Инструмент не вызывается | Mistral не поддерживает нативный tool calling | Вызываем вручную или оборачиваем сценарием |
| Ответы всё ещё короткие | Мало эпох или плохой датасет | 3–5 эпох на качественном датасете |

---

## ➡️ Что дальше

```
┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│  ДЗ 8        │ ─→  │ adapters_    │ ─→  │ 09_hw_ie_   │
│ FT + Tools   │     │ colab        │     │ extraction   │
└──────────────┘     └──────────────┘     └──────────────┘
   сейчас             переключение          извлечение
                      адаптеров             сущностей
```

**≈5 ч** — TinyLlama демо | **≈1–2 дня** — Mistral-7B полный прогон

---

## 📁 Структура файлов

| Файл | Для чего | Совет |
|------|----------|-------|
| `peft_tools_hw.ipynb` | FT → tools → интеграция | Начни с `text_formatter` — самый простой |
| `adapters_colab/` | Переключение/merge адаптеров | Для разных форматов (JSON, код) |
| `prompt_cognitive_designer.md` | Системный промпт | Опционально |

---

## 🎯 Трек C — Итог

```
┌───────────────────────────────────────────────┐
│ Вход: «Что такое рекурсия?»                  │
├───────────────────────────────────────────────┤
│ До FT: «Вызов функции самой себя»            │ ← ❌ Сухо
├───────────────────────────────────────────────┤
│ После FT: «Рекурсия — как матрёшка внутри   │
│ матрёшки. Пример: факториал 5 = 5 * факт(4)»│ ← ✅ Развёрнуто + пример
├───────────────────────────────────────────────┤
│ + Tools: «Отформатируй по 40 симв.»         │
│ → text_formatter → идеальный вывод           │ ← ✨ Делегирование
└───────────────────────────────────────────────┘
```

---

**Порядок действий:**  
1️⃣ Открой `peft_tools_hw.ipynb` в Colab с GPU  
2️⃣ Запусти установку зависимостей  
3️⃣ Пройди: базовая модель → FT → text_formatter → интеграция  
4️⃣ Проверь себя по чек-листу  

→ Готов? Начни с одного инструмента — и всё сложится. 🚀
