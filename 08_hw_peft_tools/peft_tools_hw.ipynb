{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ДЗ 8: Fine-tuning + LangChain Tools\n",
        "\n",
        "**Трек C — Инструктивный помощник**\n",
        "\n",
        "1. Fine-tuning на FineTome-100k (следование инструкциям)\n",
        "2. LangChain Tools: text_formatter, template_generator, structure_analyzer, content_validator\n",
        "3. Интеграция: fine-tuned модель + tools, сравнение до/после, связка модель+tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Установка зависимостей\n",
        "\n",
        "> ⚠️ В Colab после `pip install` может понадобиться **Runtime → Restart session**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Порядок установки: transformers + huggingface_hub должны быть совместимы (is_offline_mode)\n",
        "# langchain-huggingface НЕ ставим — он требует hf_hub<1.0, а transformers 5.x нужен >=1.3\n",
        "!pip install -q -U huggingface_hub>=1.3.0 transformers datasets peft accelerate bitsandbytes trl\n",
        "!pip install -q langchain langchain-core langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Fine-tuning (FineTome-100k + LoRA)\n",
        "\n",
        "**Выбор модели:** `tiny` — быстрый показ (~1 ч, публичная), `mistral` — качественнее (~5 ч)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# tiny = ~1 ч (Colab T4, публичная), mistral = ~5 ч, качественнее\n",
        "MODEL_MODE = \"tiny\"  # \"tiny\" | \"mistral\"\n",
        "\n",
        "MODELS = {\n",
        "    \"tiny\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",   # 1.1B, публичная, быстрый\n",
        "    \"mistral\": \"mistralai/Mistral-7B-v0.1\",\n",
        "}\n",
        "model_id = MODELS[MODEL_MODE]\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(f\"Модель: {model_id}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подготовка FineTome-100k → instruction format\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "# Размер зависит от модели: smol — 800 (~1 ч), mistral — 1500 (~5 ч)\n",
        "N_SAMPLES = 800 if MODEL_MODE == \"tiny\" else 1500\n",
        "ds = ds.select(range(N_SAMPLES))\n",
        "\n",
        "def format_conversation(sample):\n",
        "    conv = sample[\"conversations\"]\n",
        "    if len(conv) < 2:\n",
        "        return None\n",
        "    inst = next((m[\"value\"] for m in conv if m.get(\"from\") == \"human\"), \"\")\n",
        "    resp = next((m[\"value\"] for m in conv if m.get(\"from\") == \"gpt\"), \"\")\n",
        "    if not inst or not resp:\n",
        "        return {\"text\": \"\"}\n",
        "    text = f\"\"\"### Instruction:\n",
        "{inst}\n",
        "\n",
        "### Response:\n",
        "{resp}\"\"\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "ds = ds.map(format_conversation, remove_columns=ds.column_names)\n",
        "ds = ds.filter(lambda x: x[\"text\"] and len(x[\"text\"]) > 10)\n",
        "print(ds.num_rows, \"примеров\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# T4: bf16 недоступен, fp16 даёт BFloat16/scaler конфликт → fp32 без AMP\n",
        "try:\n",
        "    cap = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
        "    use_bf16 = cap[0] >= 8\n",
        "except Exception:\n",
        "    use_bf16 = False\n",
        "# На T4 fp16 даёт BFloat16/scaler конфликт → отключаем AMP\n",
        "use_amp = use_bf16\n",
        "print(\"Precision: fp32\" if not use_amp else f\"Precision: {'bf16' if use_bf16 else 'fp16'}\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=ds,\n",
        "    processing_class=tokenizer,\n",
        "    args=SFTConfig(\n",
        "        max_length=512,\n",
        "        per_device_train_batch_size=2 if not use_amp else (8 if MODEL_MODE == \"tiny\" else 4),\n",
        "        gradient_accumulation_steps=8 if not use_amp else 4,\n",
        "        warmup_steps=15,\n",
        "        max_steps=80 if MODEL_MODE == \"tiny\" else 120,\n",
        "        learning_rate=2e-4,\n",
        "        bf16=use_bf16 if use_amp else False,\n",
        "        fp16=(not use_bf16 and use_amp),\n",
        "        logging_steps=5,\n",
        "        logging_first_step=True,\n",
        "        logging_strategy=\"steps\",\n",
        "        report_to=\"none\",\n",
        "        output_dir=\"outputs_finetome\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        save_steps=40,\n",
        "        save_total_limit=2,\n",
        "        run_name=\"finetome_lora\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "n_steps = 80 if MODEL_MODE == \"tiny\" else 120\n",
        "eta = \"~1 ч\" if MODEL_MODE == \"tiny\" else \"~4–5 ч\"\n",
        "print(f\"Датасет: {len(ds)} примеров | Шагов: {n_steps} | {eta}\")\n",
        "print(\"Запуск обучения...\")\n",
        "result = trainer.train()\n",
        "print(f\"\\nГотово. Loss: {result.training_loss:.4f} | Время: {result.metrics.get('train_runtime', 0):.0f} сек\")\n",
        "trainer.save_model(\"outputs_finetome/final\")\n",
        "tokenizer.save_pretrained(\"outputs_finetome/final\")\n",
        "print(\"Модель сохранена в outputs_finetome/final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LangChain Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def text_formatter(text: str, width: int = 80, indent: int = 2) -> str:\n",
        "    \"\"\"Форматирует текст: перенос по width, отступ indent. Вход: text (строка), width (макс. символов в строке), indent (отступ).\"\"\"\n",
        "    lines = text.replace(\"\\n\", \" \").split()\n",
        "    result = []\n",
        "    current = \"\"\n",
        "    prefix = \" \" * indent\n",
        "    for w in lines:\n",
        "        if len(current) + len(w) + 1 <= width:\n",
        "            current = f\"{current} {w}\".strip() if current else w\n",
        "        else:\n",
        "            if current:\n",
        "                result.append(prefix + current)\n",
        "            current = w\n",
        "    if current:\n",
        "        result.append(prefix + current)\n",
        "    return \"\\n\".join(result) if result else text\n",
        "\n",
        "\n",
        "@tool\n",
        "def template_generator(template_type: str, placeholders: str = \"\") -> str:\n",
        "    \"\"\"Генерирует шаблон по типу. template_type: 'email'|'json'|'markdown'|'prompt'. placeholders — список полей через запятую.\"\"\"\n",
        "    templates = {\n",
        "        \"email\": \"Subject: {subject}\\n\\nDear {name},\\n\\n{body}\\n\\nBest regards,\\n{sender}\",\n",
        "        \"json\": \"{\\\"key\\\": \\\"value\\\"}\",\n",
        "        \"markdown\": \"# {title}\\n\\n## Введение\\n{intro}\\n\\n## Основная часть\\n{content}\\n\\n## Заключение\\n{conclusion}\",\n",
        "        \"prompt\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        "    }\n",
        "    return templates.get(template_type.lower(), templates[\"prompt\"])\n",
        "\n",
        "\n",
        "@tool\n",
        "def structure_analyzer(text: str) -> str:\n",
        "    \"\"\"Анализирует структуру текста: заголовки (#, ##), списки (-, *, 1.), параграфы. Возвращает описание структуры.\"\"\"\n",
        "    import json\n",
        "    desc = {\"headers\": [], \"lists\": 0, \"paragraphs\": 0, \"length_chars\": len(text)}\n",
        "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
        "    for line in lines:\n",
        "        if line.startswith(\"#\"):\n",
        "            level = len(line) - len(line.lstrip(\"#\"))\n",
        "            desc[\"headers\"].append({\"level\": level, \"text\": line.lstrip(\"# \")[:50]})\n",
        "        elif line.startswith((\"-\", \"*\", \"1.\", \"2.\")) or (len(line) > 1 and line[0].isdigit() and line[1] in \".)\"):\n",
        "            desc[\"lists\"] = desc.get(\"lists\", 0) + 1\n",
        "        elif len(line) > 20:\n",
        "            desc[\"paragraphs\"] += 1\n",
        "    return json.dumps(desc, ensure_ascii=False)\n",
        "\n",
        "\n",
        "@tool\n",
        "def content_validator(text: str, rules: str = \"structure\") -> str:\n",
        "    \"\"\"Проверяет текст по правилам. rules: 'structure' (есть заголовки/параграфы) | 'length' (не пустой) | 'format' (markdown/email). Возвращает JSON: valid (bool), issues (list).\"\"\"\n",
        "    import json\n",
        "    issues = []\n",
        "    if rules == \"length\" or \"length\" in rules:\n",
        "        if not text or not text.strip():\n",
        "            issues.append(\"Текст пустой\")\n",
        "    if rules == \"structure\" or \"structure\" in rules:\n",
        "        if \"#\" not in text and \"\\n\\n\" not in text:\n",
        "            issues.append(\"Нет чёткой структуры (заголовки/параграфы)\")\n",
        "    if rules == \"format\" or \"format\" in rules:\n",
        "        if \"@\" in text and \"Subject:\" not in text:\n",
        "            issues.append(\"Похоже на email, но нет Subject\")\n",
        "    return json.dumps({\"valid\": len(issues) == 0, \"issues\": issues}, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tools = [text_formatter, template_generator, structure_analyzer, content_validator]\n",
        "for t in tools:\n",
        "    print(t.name, \":\", t.description[:60] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Сравнение до/после fine-tuning\n",
        "\n",
        "Запускаем одинаковые промпты на базовой модели и на fine-tuned, чтобы оценить влияние обучения на следование инструкциям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Тестовые промпты для сравнения\n",
        "COMPARE_PROMPTS = [\n",
        "    \"Объясни, что такое рекурсия в программировании. Приведи короткий пример на Python.\",\n",
        "    \"Напиши короткое приветственное письмо коллеге перед отпуском.\",\n",
        "]\n",
        "\n",
        "def make_prompt(instruction):\n",
        "    return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def run_model(model, tokenizer, prompts, max_new=150):\n",
        "    results = []\n",
        "    for p in prompts:\n",
        "        full = make_prompt(p)\n",
        "        inp = tokenizer(full, return_tensors=\"pt\").to(model.device)\n",
        "        out = model.generate(**inp, max_new_tokens=max_new, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
        "        raw = tokenizer.decode(out[0][inp.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "        for stop in [\"\\n\\n### Instruction\", \"### Instruction\", \"\\n### Response\"]:\n",
        "            if stop in raw:\n",
        "                raw = raw.split(stop)[0]\n",
        "        results.append(raw.strip())\n",
        "    return results\n",
        "\n",
        "# 1) Базовая модель (без fine-tuning)\n",
        "from transformers import BitsAndBytesConfig\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "base_tok = AutoTokenizer.from_pretrained(model_id)\n",
        "base_tok.pad_token = base_tok.eos_token\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb, device_map=\"auto\")\n",
        "base_results = run_model(base_model, base_tok, COMPARE_PROMPTS)\n",
        "print(\"=== ДО fine-tuning (базовая модель) ===\")\n",
        "for i, (p, r) in enumerate(zip(COMPARE_PROMPTS, base_results)):\n",
        "    print(f\"\\n[{i+1}] Промпт: {p[:50]}...\")\n",
        "    print(f\"Ответ: {r[:300]}...\")\n",
        "del base_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Интеграция: fine-tuned модель + tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "# Загрузка fine-tuned адаптера (или базовой модели, если обучение ещё не запущено)\n",
        "adapter_path = \"outputs_finetome/final\"\n",
        "model_id = MODELS[MODEL_MODE]\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        adapter_path,\n",
        "        quantization_config=bnb,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "except Exception:\n",
        "    # Fallback: базовая модель (если адаптер не сохранён)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb, device_map=\"auto\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Fine-tuned модель — те же промпты\n",
        "finetuned_results = run_model(model, tokenizer, COMPARE_PROMPTS)\n",
        "print(\"=== ПОСЛЕ fine-tuning ===\")\n",
        "for i, (p, r) in enumerate(zip(COMPARE_PROMPTS, finetuned_results)):\n",
        "    print(f\"\\n[{i+1}] Промпт: {p[:50]}...\")\n",
        "    print(f\"Ответ: {r[:300]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mistral не поддерживает tool calling нативно — используем прямые вызовы tools\n",
        "# ReAct-агент доступен, но Mistral без tool-calling может давать нестабильный вывод.\n",
        "# Используем прямые вызовы tools (см. ячейку ниже) как основной демо-сценарий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Демо: прямой вызов модели (без tools)\n",
        "def trim_response(text):\n",
        "    \"\"\"Обрезаем, если модель начала новый блок Instruction.\"\"\"\n",
        "    for stop in [\"\\n\\n### Instruction\", \"### Instruction\", \"\\n### Response\"]:\n",
        "        if stop in text:\n",
        "            text = text.split(stop)[0]\n",
        "    return text.strip()\n",
        "\n",
        "def generate(prompt_text, max_new_tokens=200):\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
        "    raw = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return trim_response(raw)\n",
        "\n",
        "user_prompt = \"Объясни, что такое рекурсия в программировании. Приведи короткий пример на Python.\"\n",
        "full_prompt = f\"\"\"### Instruction:\n",
        "{user_prompt}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"Ответ модели:\")\n",
        "print(generate(full_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Демо: прямой вызов tools\n",
        "sample = \"Длинный текст который нужно отформатировать по ширине 40 символов с отступом 2 пробела для читаемости.\"\n",
        "print(\"text_formatter:\", text_formatter.invoke({\"text\": sample, \"width\": 40, \"indent\": 2}))\n",
        "print()\n",
        "print(\"template_generator:\", template_generator.invoke({\"template_type\": \"prompt\"}))\n",
        "print()\n",
        "print(\"structure_analyzer:\", structure_analyzer.invoke({\"text\": \"# Заголовок\\n\\n- пункт 1\\n- пункт 2\\n\\nПараграф текста.\"}))\n",
        "print()\n",
        "print(\"content_validator:\", content_validator.invoke({\"text\": \"# Заголовок\\n\\nПараграф.\", \"rules\": \"structure\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Связка модель + tools\n",
        "\n",
        "Сценарий: пользователь просит отформатировать текст → вызываем `text_formatter` → модель генерирует краткий ответ о выполненной работе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сценарий: запрос на форматирование → tool → модель подводит итог\n",
        "user_request = \"Отформатируй этот текст по ширине 50: Python это язык программирования высокого уровня. Он поддерживает объектно ориентированное программирование и имеет чистый синтаксис.\"\n",
        "raw_text = \"Python это язык программирования высокого уровня. Он поддерживает объектно ориентированное программирование и имеет чистый синтаксис.\"\n",
        "\n",
        "# 1) Вызов tool\n",
        "formatted = text_formatter.invoke({\"text\": raw_text, \"width\": 50, \"indent\": 2})\n",
        "print(\"Результат text_formatter:\\n\", formatted)\n",
        "print()\n",
        "\n",
        "# 2) Модель генерирует краткое резюме для пользователя\n",
        "summary_prompt = f\"\"\"### Instruction:\n",
        "Пользователь попросил отформатировать текст. Текст отформатирован. Кратко (1-2 предложения) сообщи пользователю, что сделано.\n",
        "\n",
        "Отформатированный текст:\n",
        "---\n",
        "{formatted}\n",
        "---\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "print(\"Ответ модели:\")\n",
        "print(generate(summary_prompt, max_new_tokens=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Опционально: системный промпт «когнитивный дизайнер»\n",
        "\n",
        "Для объяснений в стиле когнитивного дизайнера — см. `prompt_cognitive_designer.md` или `../06_prompting_guide/promt.md`. Добавь его в `full_prompt` перед запросом пользователя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример с системным промптом-когнитивным дизайнером\n",
        "# Инструкция — в формате, который модель знает: внутри Instruction\n",
        "user_q = \"Объясни, что такое замыкание в JavaScript.\"\n",
        "instruction = f\"Стиль: объясняй через аналогии из жизни, начни с «Представь...». Вопрос: {user_q}\"\n",
        "prompt_cognitive = f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"С когнитивным дизайнером:\")\n",
        "print(generate(prompt_cognitive, max_new_tokens=250))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
