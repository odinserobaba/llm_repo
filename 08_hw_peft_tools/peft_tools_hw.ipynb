{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ 8: Fine-tuning + LangChain Tools\n",
    "\n",
    "**Трек C — Инструктивный помощник**\n",
    "\n",
    "1. Fine-tuning на FineTome-100k (следование инструкциям)\n",
    "2. LangChain Tools: text_formatter, template_generator, content_validator\n",
    "3. Интеграция: fine-tuned модель + tools в агенте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Установка зависимостей\n",
    "\n",
    "> ⚠️ В Colab после `pip install` может понадобиться **Runtime → Restart session**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Порядок установки: transformers + huggingface_hub должны быть совместимы (is_offline_mode)\n",
    "# langchain-huggingface НЕ ставим — он требует hf_hub<1.0, а transformers 5.x нужен >=1.3\n",
    "!pip install -q -U huggingface_hub>=1.3.0 transformers datasets peft accelerate bitsandbytes trl\n",
    "!pip install -q langchain langchain-core langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fine-tuning (FineTome-100k + LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка FineTome-100k → instruction format\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "# Подвыборка для качественного ДЗ (~4–5 ч на Colab T4). Для быстрого теста — 500.\n",
    "ds = ds.select(range(1500))\n",
    "\n",
    "def format_conversation(sample):\n",
    "    conv = sample[\"conversations\"]\n",
    "    if len(conv) < 2:\n",
    "        return None\n",
    "    inst = next((m[\"value\"] for m in conv if m.get(\"from\") == \"human\"), \"\")\n",
    "    resp = next((m[\"value\"] for m in conv if m.get(\"from\") == \"gpt\"), \"\")\n",
    "    if not inst or not resp:\n",
    "        return {\"text\": \"\"}\n",
    "    text = f\"\"\"### Instruction:\n",
    "{inst}\n",
    "\n",
    "### Response:\n",
    "{resp}\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds = ds.map(format_conversation, remove_columns=ds.column_names)\n",
    "ds = ds.filter(lambda x: x[\"text\"] and len(x[\"text\"]) > 10)\n",
    "print(ds.num_rows, \"примеров\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds,\n",
    "    processing_class=tokenizer,\n",
    "    args=SFTConfig(\n",
    "        max_length=512,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=20,\n",
    "        max_steps=120,\n",
    "        learning_rate=2e-4,\n",
    "        bf16=True,\n",
    "        logging_steps=5,\n",
    "        logging_first_step=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        report_to=\"none\",\n",
    "        output_dir=\"outputs_finetome\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_steps=40,\n",
    "        save_total_limit=2,\n",
    "        run_name=\"finetome_lora\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Датасет: {len(ds)} примеров | Шагов: 120 | Логирование каждые 5 шагов (~4–5 ч)\")\n",
    "print(\"Запуск обучения...\")\n",
    "result = trainer.train()\n",
    "print(f\"\\nГотово. Loss: {result.training_loss:.4f} | Время: {result.metrics.get('train_runtime', 0):.0f} сек\")\n",
    "trainer.save_model(\"outputs_finetome/final\")\n",
    "tokenizer.save_pretrained(\"outputs_finetome/final\")\n",
    "print(\"Модель сохранена в outputs_finetome/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LangChain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def text_formatter(text: str, width: int = 80, indent: int = 2) -> str:\n",
    "    \"\"\"Форматирует текст: перенос по width, отступ indent. Вход: text (строка), width (макс. символов в строке), indent (отступ).\"\"\"\n",
    "    lines = text.replace(\"\\n\", \" \").split()\n",
    "    result = []\n",
    "    current = \"\"\n",
    "    prefix = \" \" * indent\n",
    "    for w in lines:\n",
    "        if len(current) + len(w) + 1 <= width:\n",
    "            current = f\"{current} {w}\".strip() if current else w\n",
    "        else:\n",
    "            if current:\n",
    "                result.append(prefix + current)\n",
    "            current = w\n",
    "    if current:\n",
    "        result.append(prefix + current)\n",
    "    return \"\\n\".join(result) if result else text\n",
    "\n",
    "\n",
    "@tool\n",
    "def template_generator(template_type: str, placeholders: str = \"\") -> str:\n",
    "    \"\"\"Генерирует шаблон по типу. template_type: 'email'|'json'|'markdown'|'prompt'. placeholders — список полей через запятую.\"\"\"\n",
    "    templates = {\n",
    "        \"email\": \"Subject: {subject}\\n\\nDear {name},\\n\\n{body}\\n\\nBest regards,\\n{sender}\",\n",
    "        \"json\": \"{\\\"key\\\": \\\"value\\\"}\",\n",
    "        \"markdown\": \"# {title}\\n\\n## Введение\\n{intro}\\n\\n## Основная часть\\n{content}\\n\\n## Заключение\\n{conclusion}\",\n",
    "        \"prompt\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "    }\n",
    "    return templates.get(template_type.lower(), templates[\"prompt\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "def content_validator(text: str, rules: str = \"structure\") -> str:\n",
    "    \"\"\"Проверяет текст по правилам. rules: 'structure' (есть заголовки/параграфы) | 'length' (не пустой) | 'format' (markdown/email). Возвращает JSON: valid (bool), issues (list).\"\"\"\n",
    "    import json\n",
    "    issues = []\n",
    "    if rules == \"length\" or \"length\" in rules:\n",
    "        if not text or not text.strip():\n",
    "            issues.append(\"Текст пустой\")\n",
    "    if rules == \"structure\" or \"structure\" in rules:\n",
    "        if \"#\" not in text and \"\\n\\n\" not in text:\n",
    "            issues.append(\"Нет чёткой структуры (заголовки/параграфы)\")\n",
    "    if rules == \"format\" or \"format\" in rules:\n",
    "        if \"@\" in text and \"Subject:\" not in text:\n",
    "            issues.append(\"Похоже на email, но нет Subject\")\n",
    "    return json.dumps({\"valid\": len(issues) == 0, \"issues\": issues}, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [text_formatter, template_generator, content_validator]\n",
    "for t in tools:\n",
    "    print(t.name, \":\", t.description[:60] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Интеграция: fine-tuned модель + tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Загрузка fine-tuned адаптера (или базовой модели, если обучение ещё не запущено)\n",
    "adapter_path = \"outputs_finetome/final\"\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        adapter_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "except Exception:\n",
    "    # Fallback: базовая модель (если адаптер не сохранён)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral не поддерживает tool calling нативно — используем прямые вызовы tools\n",
    "# ReAct-агент доступен, но Mistral без tool-calling может давать нестабильный вывод.\n",
    "# Используем прямые вызовы tools (см. ячейку ниже) как основной демо-сценарий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Демо: прямой вызов модели (без tools)\n",
    "def trim_response(text):\n",
    "    \"\"\"Обрезаем, если модель начала новый блок Instruction.\"\"\"\n",
    "    for stop in [\"\\n\\n### Instruction\", \"### Instruction\", \"\\n### Response\"]:\n",
    "        if stop in text:\n",
    "            text = text.split(stop)[0]\n",
    "    return text.strip()\n",
    "\n",
    "def generate(prompt_text, max_new_tokens=200):\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "    raw = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return trim_response(raw)\n",
    "\n",
    "user_prompt = \"Объясни, что такое рекурсия в программировании. Приведи короткий пример на Python.\"\n",
    "full_prompt = f\"\"\"### Instruction:\n",
    "{user_prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(\"Ответ модели:\")\n",
    "print(generate(full_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Демо: агент с tools (если модель поддерживает ReAct)\n",
    "# Для Mistral без tool calling может работать нестабильно — альтернатива: ручной вызов tools\n",
    "\n",
    "# Прямой вызов tools:\n",
    "sample = \"Длинный текст который нужно отформатировать по ширине 40 символов с отступом 2 пробела для читаемости.\"\n",
    "print(\"text_formatter:\", text_formatter.invoke({\"text\": sample, \"width\": 40, \"indent\": 2}))\n",
    "print()\n",
    "print(\"template_generator:\", template_generator.invoke({\"template_type\": \"prompt\"}))\n",
    "print()\n",
    "print(\"content_validator:\", content_validator.invoke({\"text\": \"# Заголовок\\n\\nПараграф.\", \"rules\": \"structure\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Опционально: системный промпт «когнитивный дизайнер»\n",
    "\n",
    "Для объяснений в стиле когнитивного дизайнера — см. `prompt_cognitive_designer.md` или `../06_prompting_guide/promt.md`. Добавь его в `full_prompt` перед запросом пользователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример с системным промптом-когнитивным дизайнером\n",
    "# Инструкция — в формате, который модель знает: внутри Instruction\n",
    "user_q = \"Объясни, что такое замыкание в JavaScript.\"\n",
    "instruction = f\"Стиль: объясняй через аналогии из жизни, начни с «Представь...». Вопрос: {user_q}\"\n",
    "prompt_cognitive = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(\"С когнитивным дизайнером:\")\n",
    "print(generate(prompt_cognitive, max_new_tokens=250))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
