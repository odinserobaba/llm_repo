{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –î–ó 8: Fine-tuning + LangChain Tools\n",
        "\n",
        "**–¢—Ä–µ–∫ C ‚Äî –ò–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫**\n",
        "\n",
        "1. Fine-tuning –Ω–∞ FineTome-100k (—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º)\n",
        "2. LangChain Tools: text_formatter, template_generator, structure_analyzer, content_validator\n",
        "3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: fine-tuned –º–æ–¥–µ–ª—å + tools, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ/–ø–æ—Å–ª–µ, —Å–≤—è–∑–∫–∞ –º–æ–¥–µ–ª—å+tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üéØ –Ø–∫–æ—Ä—å\n",
        "–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ + 1000 –ø—Ä–∏–º–µ—Ä–æ–≤ ¬´–≤–æ–ø—Ä–æ—Å ‚Üí –æ—Ç–≤–µ—Ç¬ª + ¬´—Ä—É–∫–∏¬ª (tools). –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—É –∏ –¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º.\n",
        "\n",
        "**–¢–µ—Ä–º–∏–Ω—ã:** Mistral-7B / TinyLlama | FineTome-100k | LoRA 4-bit | text_formatter, template_generator | OOM = –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç VRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "\n",
        "> ‚ö†Ô∏è –í Colab –ø–æ—Å–ª–µ `pip install` –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è **Runtime ‚Üí Restart session**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ü–æ—Ä—è–¥–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–∫–∏: transformers + huggingface_hub –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã (is_offline_mode)\n",
        "# langchain-huggingface –ù–ï —Å—Ç–∞–≤–∏–º ‚Äî –æ–Ω —Ç—Ä–µ–±—É–µ—Ç hf_hub<1.0, –∞ transformers 5.x –Ω—É–∂–µ–Ω >=1.3\n",
        "!pip install -q -U huggingface_hub>=1.3.0 transformers datasets peft accelerate bitsandbytes trl\n",
        "!pip install -q langchain langchain-core langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Fine-tuning (FineTome-100k + LoRA)\n",
        "\n",
        "**–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏:** `tiny` ‚Äî –±—ã—Å—Ç—Ä—ã–π –ø–æ–∫–∞–∑ (~1 —á, –ø—É–±–ª–∏—á–Ω–∞—è), `mistral` ‚Äî –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ (~5 —á)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# tiny = ~1 —á (Colab T4, –ø—É–±–ª–∏—á–Ω–∞—è), mistral = ~5 —á, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ\n",
        "MODEL_MODE = \"tiny\"  # \"tiny\" | \"mistral\"\n",
        "\n",
        "MODELS = {\n",
        "    \"tiny\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",   # 1.1B, –ø—É–±–ª–∏—á–Ω–∞—è, –±—ã—Å—Ç—Ä—ã–π\n",
        "    \"mistral\": \"mistralai/Mistral-7B-v0.1\",\n",
        "}\n",
        "model_id = MODELS[MODEL_MODE]\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(f\"–ú–æ–¥–µ–ª—å: {model_id}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### ‚ú® –ü—Ä–æ—Ä—ã–≤ (–∑–∞–≥—Ä—É–∑–∫–∞)\n",
        "–ú–æ–¥–µ–ª—å –≤ **4-bit** ‚Äî –≤–ª–µ–∑–∞–µ—Ç –≤ Colab T4. LoRA ‚Äî –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ ~0.05% –≤–µ—Å–æ–≤. –ê–¥–∞–ø—Ç–µ—Ä –±—É–¥–µ—Ç ~50 MB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ FineTome-100k ‚Üí instruction format\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "# –†–∞–∑–º–µ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏: smol ‚Äî 800 (~1 —á), mistral ‚Äî 1500 (~5 —á)\n",
        "N_SAMPLES = 800 if MODEL_MODE == \"tiny\" else 1500\n",
        "ds = ds.select(range(N_SAMPLES))\n",
        "\n",
        "def format_conversation(sample):\n",
        "    conv = sample[\"conversations\"]\n",
        "    if len(conv) < 2:\n",
        "        return None\n",
        "    inst = next((m[\"value\"] for m in conv if m.get(\"from\") == \"human\"), \"\")\n",
        "    resp = next((m[\"value\"] for m in conv if m.get(\"from\") == \"gpt\"), \"\")\n",
        "    if not inst or not resp:\n",
        "        return {\"text\": \"\"}\n",
        "    text = f\"\"\"### Instruction:\n",
        "{inst}\n",
        "\n",
        "### Response:\n",
        "{resp}\"\"\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "ds = ds.map(format_conversation, remove_columns=ds.column_names)\n",
        "ds = ds.filter(lambda x: x[\"text\"] and len(x[\"text\"]) > 10)\n",
        "print(ds.num_rows, \"–ø—Ä–∏–º–µ—Ä–æ–≤\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# T4: bf16 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, fp16 –¥–∞—ë—Ç BFloat16/scaler –∫–æ–Ω—Ñ–ª–∏–∫—Ç ‚Üí fp32 –±–µ–∑ AMP\n",
        "try:\n",
        "    cap = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
        "    use_bf16 = cap[0] >= 8\n",
        "except Exception:\n",
        "    use_bf16 = False\n",
        "# –ù–∞ T4 fp16 –¥–∞—ë—Ç BFloat16/scaler –∫–æ–Ω—Ñ–ª–∏–∫—Ç ‚Üí –æ—Ç–∫–ª—é—á–∞–µ–º AMP\n",
        "use_amp = use_bf16\n",
        "print(\"Precision: fp32\" if not use_amp else f\"Precision: {'bf16' if use_bf16 else 'fp16'}\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=ds,\n",
        "    processing_class=tokenizer,\n",
        "    args=SFTConfig(\n",
        "        max_length=512,\n",
        "        per_device_train_batch_size=2 if not use_amp else (8 if MODEL_MODE == \"tiny\" else 4),\n",
        "        gradient_accumulation_steps=8 if not use_amp else 4,\n",
        "        warmup_steps=15,\n",
        "        max_steps=80 if MODEL_MODE == \"tiny\" else 120,\n",
        "        learning_rate=2e-4,\n",
        "        bf16=use_bf16 if use_amp else False,\n",
        "        fp16=(not use_bf16 and use_amp),\n",
        "        logging_steps=5,\n",
        "        logging_first_step=True,\n",
        "        logging_strategy=\"steps\",\n",
        "        report_to=\"none\",\n",
        "        output_dir=\"outputs_finetome\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        save_steps=40,\n",
        "        save_total_limit=2,\n",
        "        run_name=\"finetome_lora\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "n_steps = 80 if MODEL_MODE == \"tiny\" else 120\n",
        "eta = \"~1 —á\" if MODEL_MODE == \"tiny\" else \"~4‚Äì5 —á\"\n",
        "print(f\"–î–∞—Ç–∞—Å–µ—Ç: {len(ds)} –ø—Ä–∏–º–µ—Ä–æ–≤ | –®–∞–≥–æ–≤: {n_steps} | {eta}\")\n",
        "print(\"–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...\")\n",
        "result = trainer.train()\n",
        "print(f\"\\n–ì–æ—Ç–æ–≤–æ. Loss: {result.training_loss:.4f} | –í—Ä–µ–º—è: {result.metrics.get('train_runtime', 0):.0f} —Å–µ–∫\")\n",
        "trainer.save_model(\"outputs_finetome/final\")\n",
        "tokenizer.save_pretrained(\"outputs_finetome/final\")\n",
        "print(\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ outputs_finetome/final\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### ‚ú® –ü—Ä–æ—Ä—ã–≤ (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)\n",
        "–ú–æ–¥–µ–ª—å **–Ω–∞—É—á–∏–ª–∞—Å—å** —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ –æ—Ç–≤–µ—á–∞—Ç—å –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º FineTome. –ê–¥–∞–ø—Ç–µ—Ä ~50 MB —Å–æ—Ö—Ä–∞–Ω—ë–Ω ‚Äî –Ω–µ –ø–µ—Ä–µ–ø–∏—Å–∞–ª–∏ –≤—Å–µ 7B –≤–µ—Å–æ–≤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LangChain Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def text_formatter(text: str, width: int = 80, indent: int = 2) -> str:\n",
        "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç: –ø–µ—Ä–µ–Ω–æ—Å –ø–æ width, –æ—Ç—Å—Ç—É–ø indent. –í—Ö–æ–¥: text (—Å—Ç—Ä–æ–∫–∞), width (–º–∞–∫—Å. —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ), indent (–æ—Ç—Å—Ç—É–ø).\"\"\"\n",
        "    lines = text.replace(\"\\n\", \" \").split()\n",
        "    result = []\n",
        "    current = \"\"\n",
        "    prefix = \" \" * indent\n",
        "    for w in lines:\n",
        "        if len(current) + len(w) + 1 <= width:\n",
        "            current = f\"{current} {w}\".strip() if current else w\n",
        "        else:\n",
        "            if current:\n",
        "                result.append(prefix + current)\n",
        "            current = w\n",
        "    if current:\n",
        "        result.append(prefix + current)\n",
        "    return \"\\n\".join(result) if result else text\n",
        "\n",
        "\n",
        "@tool\n",
        "def template_generator(template_type: str, placeholders: str = \"\") -> str:\n",
        "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —à–∞–±–ª–æ–Ω –ø–æ —Ç–∏–ø—É. template_type: 'email'|'json'|'markdown'|'prompt'. placeholders ‚Äî —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.\"\"\"\n",
        "    templates = {\n",
        "        \"email\": \"Subject: {subject}\\n\\nDear {name},\\n\\n{body}\\n\\nBest regards,\\n{sender}\",\n",
        "        \"json\": \"{\\\"key\\\": \\\"value\\\"}\",\n",
        "        \"markdown\": \"# {title}\\n\\n## –í–≤–µ–¥–µ–Ω–∏–µ\\n{intro}\\n\\n## –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å\\n{content}\\n\\n## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\\n{conclusion}\",\n",
        "        \"prompt\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        "    }\n",
        "    return templates.get(template_type.lower(), templates[\"prompt\"])\n",
        "\n",
        "\n",
        "@tool\n",
        "def structure_analyzer(text: str) -> str:\n",
        "    \"\"\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞: –∑–∞–≥–æ–ª–æ–≤–∫–∏ (#, ##), —Å–ø–∏—Å–∫–∏ (-, *, 1.), –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.\"\"\"\n",
        "    import json\n",
        "    desc = {\"headers\": [], \"lists\": 0, \"paragraphs\": 0, \"length_chars\": len(text)}\n",
        "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
        "    for line in lines:\n",
        "        if line.startswith(\"#\"):\n",
        "            level = len(line) - len(line.lstrip(\"#\"))\n",
        "            desc[\"headers\"].append({\"level\": level, \"text\": line.lstrip(\"# \")[:50]})\n",
        "        elif line.startswith((\"-\", \"*\", \"1.\", \"2.\")) or (len(line) > 1 and line[0].isdigit() and line[1] in \".)\"):\n",
        "            desc[\"lists\"] = desc.get(\"lists\", 0) + 1\n",
        "        elif len(line) > 20:\n",
        "            desc[\"paragraphs\"] += 1\n",
        "    return json.dumps(desc, ensure_ascii=False)\n",
        "\n",
        "\n",
        "@tool\n",
        "def content_validator(text: str, rules: str = \"structure\") -> str:\n",
        "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º. rules: 'structure' (–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏/–ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã) | 'length' (–Ω–µ –ø—É—Å—Ç–æ–π) | 'format' (markdown/email). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON: valid (bool), issues (list).\"\"\"\n",
        "    import json\n",
        "    issues = []\n",
        "    if rules == \"length\" or \"length\" in rules:\n",
        "        if not text or not text.strip():\n",
        "            issues.append(\"–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π\")\n",
        "    if rules == \"structure\" or \"structure\" in rules:\n",
        "        if \"#\" not in text and \"\\n\\n\" not in text:\n",
        "            issues.append(\"–ù–µ—Ç —á—ë—Ç–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–∑–∞–≥–æ–ª–æ–≤–∫–∏/–ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã)\")\n",
        "    if rules == \"format\" or \"format\" in rules:\n",
        "        if \"@\" in text and \"Subject:\" not in text:\n",
        "            issues.append(\"–ü–æ—Ö–æ–∂–µ –Ω–∞ email, –Ω–æ –Ω–µ—Ç Subject\")\n",
        "    return json.dumps({\"valid\": len(issues) == 0, \"issues\": issues}, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tools = [text_formatter, template_generator, structure_analyzer, content_validator]\n",
        "for t in tools:\n",
        "    print(t.name, \":\", t.description[:60] + \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ/–ø–æ—Å–ª–µ fine-tuning\n",
        "\n",
        "–ó–∞–ø—É—Å–∫–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã –Ω–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –Ω–∞ fine-tuned, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gc\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "COMPARE_PROMPTS = [\n",
        "    \"–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ —Ä–µ–∫—É—Ä—Å–∏—è –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü—Ä–∏–≤–µ–¥–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–∏–º–µ—Ä –Ω–∞ Python.\",\n",
        "    \"–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–∏—Å—å–º–æ –∫–æ–ª–ª–µ–≥–µ –ø–µ—Ä–µ–¥ –æ—Ç–ø—É—Å–∫–æ–º.\",\n",
        "]\n",
        "\n",
        "def make_prompt(instruction):\n",
        "    return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def run_model(model, tokenizer, prompts, max_new=150):\n",
        "    results = []\n",
        "    for p in prompts:\n",
        "        full = make_prompt(p)\n",
        "        inp = tokenizer(full, return_tensors=\"pt\").to(model.device)\n",
        "        out = model.generate(**inp, max_new_tokens=max_new, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
        "        raw = tokenizer.decode(out[0][inp.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "        for stop in [\"\\n\\n### Instruction\", \"### Instruction\", \"\\n### Response\"]:\n",
        "            if stop in raw:\n",
        "                raw = raw.split(stop)[0]\n",
        "        results.append(raw.strip())\n",
        "    return results\n",
        "\n",
        "# 1) –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–±–µ–∑ fine-tuning)\n",
        "from transformers import BitsAndBytesConfig\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "base_tok = AutoTokenizer.from_pretrained(model_id)\n",
        "base_tok.pad_token = base_tok.eos_token\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb, device_map=\"auto\")\n",
        "base_results = run_model(base_model, base_tok, COMPARE_PROMPTS)\n",
        "print(\"=== –î–û fine-tuning (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å) ===\")\n",
        "for i, (p, r) in enumerate(zip(COMPARE_PROMPTS, base_results)):\n",
        "    print(f\"\\n[{i+1}] –ü—Ä–æ–º–ø—Ç: {p[:50]}...\")\n",
        "    print(f\"–û—Ç–≤–µ—Ç: {r[:300]}...\")\n",
        "del base_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: fine-tuned –º–æ–¥–µ–ª—å + tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ fine-tuned –∞–¥–∞–ø—Ç–µ—Ä–∞ (–∏–ª–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –µ—â—ë –Ω–µ –∑–∞–ø—É—â–µ–Ω–æ)\n",
        "adapter_path = \"outputs_finetome/final\"\n",
        "model_id = MODELS[MODEL_MODE]\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        adapter_path,\n",
        "        quantization_config=bnb,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "except Exception:\n",
        "    # Fallback: –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb, device_map=\"auto\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2) Fine-tuned –º–æ–¥–µ–ª—å ‚Äî —Ç–µ –∂–µ –ø—Ä–æ–º–ø—Ç—ã\n",
        "finetuned_results = run_model(model, tokenizer, COMPARE_PROMPTS)\n",
        "print(\"=== –ü–û–°–õ–ï fine-tuning ===\")\n",
        "for i, (p, r) in enumerate(zip(COMPARE_PROMPTS, finetuned_results)):\n",
        "    print(f\"\\n[{i+1}] –ü—Ä–æ–º–ø—Ç: {p[:50]}...\")\n",
        "    print(f\"–û—Ç–≤–µ—Ç: {r[:300]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mistral –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç tool calling –Ω–∞—Ç–∏–≤–Ω–æ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—ã–µ –≤—ã–∑–æ–≤—ã tools\n",
        "# ReAct-–∞–≥–µ–Ω—Ç –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ Mistral –±–µ–∑ tool-calling –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –≤—ã–≤–æ–¥.\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—ã–µ –≤—ã–∑–æ–≤—ã tools (—Å–º. —è—á–µ–π–∫—É –Ω–∏–∂–µ) –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–µ–º–æ-—Å—Ü–µ–Ω–∞—Ä–∏–π."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –î–µ–º–æ: –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ (–±–µ–∑ tools)\n",
        "def trim_response(text):\n",
        "    \"\"\"–û–±—Ä–µ–∑–∞–µ–º, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞—á–∞–ª–∞ –Ω–æ–≤—ã–π –±–ª–æ–∫ Instruction.\"\"\"\n",
        "    for stop in [\"\\n\\n### Instruction\", \"### Instruction\", \"\\n### Response\"]:\n",
        "        if stop in text:\n",
        "            text = text.split(stop)[0]\n",
        "    return text.strip()\n",
        "\n",
        "def generate(prompt_text, max_new_tokens=200):\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
        "    raw = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return trim_response(raw)\n",
        "\n",
        "user_prompt = \"–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ —Ä–µ–∫—É—Ä—Å–∏—è –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü—Ä–∏–≤–µ–¥–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–∏–º–µ—Ä –Ω–∞ Python.\"\n",
        "full_prompt = f\"\"\"### Instruction:\n",
        "{user_prompt}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\")\n",
        "print(generate(full_prompt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –î–µ–º–æ: –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ tools\n",
        "sample = \"–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —à–∏—Ä–∏–Ω–µ 40 —Å–∏–º–≤–æ–ª–æ–≤ —Å –æ—Ç—Å—Ç—É–ø–æ–º 2 –ø—Ä–æ–±–µ–ª–∞ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏.\"\n",
        "print(\"text_formatter:\", text_formatter.invoke({\"text\": sample, \"width\": 40, \"indent\": 2}))\n",
        "print()\n",
        "print(\"template_generator:\", template_generator.invoke({\"template_type\": \"prompt\"}))\n",
        "print()\n",
        "print(\"structure_analyzer:\", structure_analyzer.invoke({\"text\": \"# –ó–∞–≥–æ–ª–æ–≤–æ–∫\\n\\n- –ø—É–Ω–∫—Ç 1\\n- –ø—É–Ω–∫—Ç 2\\n\\n–ü–∞—Ä–∞–≥—Ä–∞—Ñ —Ç–µ–∫—Å—Ç–∞.\"}))\n",
        "print()\n",
        "print(\"content_validator:\", content_validator.invoke({\"text\": \"# –ó–∞–≥–æ–ª–æ–≤–æ–∫\\n\\n–ü–∞—Ä–∞–≥—Ä–∞—Ñ.\", \"rules\": \"structure\"}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 –°–≤—è–∑–∫–∞ –º–æ–¥–µ–ª—å + tools\n",
        "\n",
        "–°—Ü–µ–Ω–∞—Ä–∏–π: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç ‚Üí –≤—ã–∑—ã–≤–∞–µ–º `text_formatter` ‚Üí –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –°—Ü–µ–Ω–∞—Ä–∏–π: –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Üí tool ‚Üí –º–æ–¥–µ–ª—å –ø–æ–¥–≤–æ–¥–∏—Ç –∏—Ç–æ–≥\n",
        "user_request = \"–û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –ø–æ —à–∏—Ä–∏–Ω–µ 50: Python —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è. –û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—ä–µ–∫—Ç–Ω–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏–º–µ–µ—Ç —á–∏—Å—Ç—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å.\"\n",
        "raw_text = \"Python —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è. –û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—ä–µ–∫—Ç–Ω–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏–º–µ–µ—Ç —á–∏—Å—Ç—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å.\"\n",
        "\n",
        "# 1) –í—ã–∑–æ–≤ tool\n",
        "formatted = text_formatter.invoke({\"text\": raw_text, \"width\": 50, \"indent\": 2})\n",
        "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç text_formatter:\\n\", formatted)\n",
        "print()\n",
        "\n",
        "# 2) –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "summary_prompt = f\"\"\"### Instruction:\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–ø—Ä–æ—Å–∏–ª –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç. –¢–µ–∫—Å—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω. –ö—Ä–∞—Ç–∫–æ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å–æ–æ–±—â–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —á—Ç–æ —Å–¥–µ–ª–∞–Ω–æ.\n",
        "\n",
        "–û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
        "---\n",
        "{formatted}\n",
        "---\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "print(\"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\")\n",
        "print(generate(summary_prompt, max_new_tokens=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç ¬´–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω–µ—Ä¬ª\n",
        "\n",
        "–î–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –≤ —Å—Ç–∏–ª–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –¥–∏–∑–∞–π–Ω–µ—Ä–∞ ‚Äî —Å–º. `prompt_cognitive_designer.md` –∏–ª–∏ `../06_prompting_guide/promt.md`. –î–æ–±–∞–≤—å –µ–≥–æ –≤ `full_prompt` –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ü—Ä–∏–º–µ—Ä —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º-–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –¥–∏–∑–∞–π–Ω–µ—Ä–æ–º\n",
        "# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è ‚Äî –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –º–æ–¥–µ–ª—å –∑–Ω–∞–µ—Ç: –≤–Ω—É—Ç—Ä–∏ Instruction\n",
        "user_q = \"–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ –≤ JavaScript.\"\n",
        "instruction = f\"–°—Ç–∏–ª—å: –æ–±—ä—è—Å–Ω—è–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–æ–≥–∏–∏ –∏–∑ –∂–∏–∑–Ω–∏, –Ω–∞—á–Ω–∏ —Å ¬´–ü—Ä–µ–¥—Å—Ç–∞–≤—å...¬ª. –í–æ–ø—Ä–æ—Å: {user_q}\"\n",
        "prompt_cognitive = f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"–° –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –¥–∏–∑–∞–π–Ω–µ—Ä–æ–º:\")\n",
        "print(generate(prompt_cognitive, max_new_tokens=250))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}