# Модуль 2 — Chains: карта понимания

Это как конвейер на кухне: помыл → нарезал → подал.

Здесь подробная и понятная карта модуля 2: что мы делали и зачем.

---

## 1) Зачем этот модуль

- строить цепочки из шагов  
- выбирать путь для разных запросов  
- делать отказоустойчивость  
- показывать ответ «живьём»  

---

## 2) Блок A — последовательные цепочки

### `01_sequential_chain.ipynb`
**Якорь:** как собрать бургер по шагам.  

**Что делаем:**
- очищаем текст  
- делаем суммаризацию  
- классифицируем  
- проверяем результат  

**Мини‑схема**
```
Очистка → Сумма → Тег
```

**Микро‑упражнение:**  
Поменяй порядок шагов и посмотри, что сломается.

**Почему важно:** без цепочки сложная задача распадается.

---

## 3) Блок B — маршрутизация

### `02_router_chain.ipynb`
**Якорь:** как диспетчер отправляет машину туда, где она нужна.  

**Что делаем:**
- ищем ключевые слова  
- выбираем ветку  
- делаем fallback  
- проверяем корректность  

**Мини‑схема**
```
Запрос → Роутер → Ветка
```

**Микро‑упражнение:**  
Добавь новую ветку «биллинг» и протестируй запрос.

**Почему важно:** разные запросы требуют разных промптов.

---

## 4) Блок C — устойчивость и поток

### `03_fallbacks_streaming.ipynb`
**Якорь:** как запасной фонарик, если погас свет.  

**Что делаем:**
- задаём основную модель  
- добавляем fallback  
- включаем stream  
- проверяем UX  

**Мини‑схема**
```
Модель A → ошибка → Модель B
```

**Микро‑упражнение:**  
Поменяй местами модели и сравни скорость.

**Почему важно:** пользователь получает ответ даже при сбоях.

---

## 5) Мини‑примеры кода (ещё подробнее)

### Пример 1: Простая цепочка LCEL
```python
chain = clean_prompt | llm | summarize_prompt | llm
# Схема: шаг → шаг → шаг
```
**Почему важно:** LCEL делает пайплайн читаемым.

### Пример 2: RouterBranch
```python
branch = RunnableBranch(
    (is_support, support_chain),
    (is_sales, sales_chain),
    default_chain,
)
# Схема: условие → ветка
```
**Почему важно:** управление логикой без громоздких if/else.

### Пример 3: Fallbacks
```python
safe_llm = primary_llm.with_fallbacks([backup_llm])
# Схема: основной → запасной
```
**Почему важно:** снижает риск «пустого ответа».

### Пример 4: Streaming
```python
for chunk in llm.stream("Привет!"):
    print(chunk.content, end="")
# Схема: токены → экран
```
**Почему важно:** быстрый отклик улучшает UX.

---

## 6) Микро‑проверка (с ответом)

Вопрос: что делает RouterChain?  
Ответ: выбирает правильную цепочку для запроса.

---

## 7) Итог модуля (4 результата)

- строишь цепочки шагов  
- делаешь маршрутизацию  
- добавляешь fallback  
- включаешь streaming  

---

Попробуй: открой `01_sequential_chain.ipynb` и пройди его сверху вниз.
