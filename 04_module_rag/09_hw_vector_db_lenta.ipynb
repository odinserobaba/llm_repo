{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eebd707",
   "metadata": {},
   "source": [
    "# ДЗ: Векторная БД на русском датасете (Lenta.ru)\n",
    "\n",
    "**Цель:** повторить задание на русскоязычных данных.\n",
    "\n",
    "**Что будет в ноутбуке:**\n",
    "- загрузка Lenta.ru CSV\n",
    "- эмбеддинги и индексация\n",
    "- базовый поиск и фильтрация по темам\n",
    "- сравнение качества/скорости и график\n",
    "\n",
    "> Используем CSV без `datasets/pandas`, чтобы избежать конфликтов в Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U \\\n",
    "  langchain \\\n",
    "  langchain-community \\\n",
    "  langchain-openai \\\n",
    "  chromadb \\\n",
    "  rank-bm25 \\\n",
    "  matplotlib \\\n",
    "  sentence-transformers \\\n",
    "  pydantic==2.12.3 \\\n",
    "  requests==2.32.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09576dd",
   "metadata": {},
   "source": [
    "## Если Colab ругается на зависимости\n",
    "\n",
    "Иногда в Colab установлены библиотеки с жёсткими версиями (например, `google-adk` и `opentelemetry`).\n",
    "Если после установки появляются конфликты — зафиксируйте совместимые версии ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U \\\n",
    "  opentelemetry-api==1.37.0 \\\n",
    "  opentelemetry-sdk==1.37.0 \\\n",
    "  opentelemetry-proto==1.37.0 \\\n",
    "  opentelemetry-exporter-otlp-proto-common==1.37.0 \\\n",
    "  opentelemetry-exporter-otlp-proto-grpc==1.37.0\n",
    "\n",
    "# На случай, если в окружении уже стоит более новая версия\n",
    "def _force_pins():\n",
    "    import sys, subprocess\n",
    "    pkgs = [\n",
    "        \"opentelemetry-api==1.37.0\",\n",
    "        \"opentelemetry-sdk==1.37.0\",\n",
    "        \"opentelemetry-proto==1.37.0\",\n",
    "        \"opentelemetry-exporter-otlp-proto-common==1.37.0\",\n",
    "        \"opentelemetry-exporter-otlp-proto-grpc==1.37.0\",\n",
    "    ]\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"--force-reinstall\"] + pkgs)\n",
    "\n",
    "_force_pins()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff055953",
   "metadata": {},
   "source": [
    "## Настройка ключа и base URL\n",
    "\n",
    "Для AITunnel укажите `OPENAI_API_KEY` и `OPENAI_BASE_URL`. В Colab лучше хранить ключ в переменной окружения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db076a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Введите OPENAI_API_KEY: \")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_BASE_URL\"):\n",
    "    os.environ[\"OPENAI_BASE_URL\"] = \"https://api.aitunnel.ru/v1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32331335",
   "metadata": {},
   "source": [
    "## Выбор датасета и эмбеддингов\n",
    "\n",
    "По умолчанию используем `ru_news` и **локальные эмбеддинги** (без API), чтобы избежать ошибки `No embedding data received`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_MODE = \"ru_news\"  # ru_news | lenta\n",
    "EMBEDDINGS_BACKEND = \"hf\"  # hf | openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aacfd7",
   "metadata": {},
   "source": [
    "## Часть 1. Настройка и индексация\n",
    "\n",
    "### 1.1 Выбор БД\n",
    "Используем **Chroma** (ANN‑индекс HNSW).\n",
    "\n",
    "### 1.2 Подготовка датасета (ru_news по умолчанию)\n",
    "Датасет выбирается в `DATASET_MODE`.\n",
    "- `ru_news` — новости из нескольких источников\n",
    "- `lenta` — Lenta.ru (если доступна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c831c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установки для ru_news (нужны только если DATASET_MODE == \"ru_news\")\n",
    "%pip -q install -U huggingface_hub zstandard jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b521b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "N_DOCS = 2000  # можно увеличить\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "def iter_rows_from_text(text: str):\n",
    "    reader = csv.DictReader(io.StringIO(text))\n",
    "    for row in reader:\n",
    "        yield row\n",
    "\n",
    "\n",
    "def fill_from_rows(rows):\n",
    "    for row in rows:\n",
    "        title = (row.get(\"title\") or \"\").strip()\n",
    "        body = (row.get(\"text\") or \"\").strip()\n",
    "        topic = (row.get(\"topic\") or \"unknown\").strip()\n",
    "        if not body:\n",
    "            continue\n",
    "        text = f\"{title}. {body}\" if title else body\n",
    "        texts.append(text)\n",
    "        labels.append(topic)\n",
    "        if len(texts) >= N_DOCS:\n",
    "            break\n",
    "\n",
    "\n",
    "def fill_from_ru_news_jsonl(lines):\n",
    "    for line in lines:\n",
    "        obj = json.loads(line)\n",
    "        title = (obj.get(\"title\") or \"\").strip()\n",
    "        body = (obj.get(\"text\") or \"\").strip()\n",
    "        source = (obj.get(\"source\") or \"unknown\").strip()\n",
    "        if not body:\n",
    "            continue\n",
    "        texts.append(f\"{title}. {body}\" if title else body)\n",
    "        labels.append(source)\n",
    "        if len(texts) >= N_DOCS:\n",
    "            break\n",
    "\n",
    "if DATASET_MODE == \"ru_news\":\n",
    "    # ru_news: загрузка .jsonl/.jsonl.zst через HF Hub (без datasets)\n",
    "    import zstandard as zstd\n",
    "    import jsonlines\n",
    "    from huggingface_hub import HfApi, hf_hub_download\n",
    "\n",
    "    REPO_ID = \"IlyaGusev/ru_news\"\n",
    "    api = HfApi()\n",
    "    files = api.list_repo_files(repo_id=REPO_ID, repo_type=\"dataset\")\n",
    "    data_file = next((f for f in files if f.endswith(\".jsonl.zst\")), None)\n",
    "    if data_file is None:\n",
    "        data_file = next((f for f in files if f.endswith(\".jsonl\")), None)\n",
    "    if data_file is None:\n",
    "        raise RuntimeError(\"Не найден файл .jsonl/.jsonl.zst в ru_news\")\n",
    "\n",
    "    path = hf_hub_download(repo_id=REPO_ID, repo_type=\"dataset\", filename=data_file)\n",
    "\n",
    "    if data_file.endswith(\".zst\"):\n",
    "        with open(path, \"rb\") as fh:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            with dctx.stream_reader(fh) as reader:\n",
    "                text_stream = io.TextIOWrapper(reader, encoding=\"utf-8\")\n",
    "                fill_from_ru_news_jsonl(text_stream)\n",
    "    else:\n",
    "        with jsonlines.open(path) as reader:\n",
    "            for obj in reader:\n",
    "                title = (obj.get(\"title\") or \"\").strip()\n",
    "                body = (obj.get(\"text\") or \"\").strip()\n",
    "                source = (obj.get(\"source\") or \"unknown\").strip()\n",
    "                if not body:\n",
    "                    continue\n",
    "                texts.append(f\"{title}. {body}\" if title else body)\n",
    "                labels.append(source)\n",
    "                if len(texts) >= N_DOCS:\n",
    "                    break\n",
    "\n",
    "    source_name = \"ru_news\"\n",
    "else:\n",
    "    # lenta.ru (CSV)\n",
    "    # Источник: https://github.com/yutkin/Lenta.Ru-News-Dataset\n",
    "    urls = [\n",
    "        \"https://raw.githubusercontent.com/yutkin/Lenta.Ru-News-Dataset/master/lenta-ru-news.csv\",\n",
    "        \"https://raw.githubusercontent.com/yutkin/Lenta.Ru-News-Dataset/main/lenta-ru-news.csv\",\n",
    "        \"https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv\",\n",
    "        \"https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.gz\",\n",
    "    ]\n",
    "\n",
    "    local_csv = Path(\"./data/lenta-ru-news.csv\")\n",
    "    local_gz = Path(\"./data/lenta-ru-news.csv.gz\")\n",
    "\n",
    "    if local_csv.exists():\n",
    "        fill_from_rows(iter_rows_from_text(local_csv.read_text(encoding=\"utf-8\", errors=\"ignore\")))\n",
    "        print(\"Используем локальный CSV:\", local_csv)\n",
    "    elif local_gz.exists():\n",
    "        with gzip.open(local_gz, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            fill_from_rows(iter_rows_from_text(f.read()))\n",
    "        print(\"Используем локальный CSV.GZ:\", local_gz)\n",
    "    else:\n",
    "        resp = None\n",
    "        last_err = None\n",
    "        for url in urls:\n",
    "            try:\n",
    "                r = requests.get(url, timeout=120)\n",
    "                r.raise_for_status()\n",
    "                if r.text.strip():\n",
    "                    resp = r\n",
    "                    print(\"Скачано из:\", url)\n",
    "                    break\n",
    "            except Exception as exc:\n",
    "                last_err = exc\n",
    "\n",
    "        if resp is None:\n",
    "            raise RuntimeError(\n",
    "                \"Не удалось скачать датасет.\\n\"\n",
    "                \"Загрузите файл вручную в ./data/lenta-ru-news.csv или ./data/lenta-ru-news.csv.gz.\\n\"\n",
    "                f\"Последняя ошибка: {last_err}\"\n",
    "            )\n",
    "\n",
    "        if resp.url.endswith(\".gz\"):\n",
    "            with gzip.open(io.BytesIO(resp.content), \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                fill_from_rows(iter_rows_from_text(f.read()))\n",
    "        else:\n",
    "            fill_from_rows(iter_rows_from_text(resp.text))\n",
    "\n",
    "    source_name = \"lenta_ru\"\n",
    "\n",
    "metadatas = [{\"label\": lbl, \"source\": source_name, \"source_id\": i} for i, lbl in enumerate(labels)]\n",
    "ids = [f\"doc-{i}\" for i in range(len(texts))]\n",
    "\n",
    "print(\"Документов:\", len(texts))\n",
    "print(\"Пример:\", texts[0][:200])\n",
    "print(\"Топик пример:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7006dad",
   "metadata": {},
   "source": [
    "### Альтернатива: датасет ru_news (Hugging Face)\n",
    "\n",
    "Если Lenta.ru недоступна, можно взять `IlyaGusev/ru_news` и загрузить **без datasets/pandas**.\n",
    "Этот блок заменит `texts/labels/metadatas/ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc279121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: ru_news\n",
    "%pip -q install -U huggingface_hub zstandard jsonlines\n",
    "\n",
    "import io\n",
    "import json\n",
    "import zstandard as zstd\n",
    "import jsonlines\n",
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "\n",
    "REPO_ID = \"IlyaGusev/ru_news\"\n",
    "N_DOCS = 2000\n",
    "\n",
    "api = HfApi()\n",
    "files = api.list_repo_files(repo_id=REPO_ID, repo_type=\"dataset\")\n",
    "\n",
    "# Ищем файл данных\n",
    "data_file = next((f for f in files if f.endswith(\".jsonl.zst\")), None)\n",
    "if data_file is None:\n",
    "    data_file = next((f for f in files if f.endswith(\".jsonl\")), None)\n",
    "\n",
    "if data_file is None:\n",
    "    raise RuntimeError(\"Не найден файл .jsonl/.jsonl.zst в ru_news\")\n",
    "\n",
    "path = hf_hub_download(repo_id=REPO_ID, repo_type=\"dataset\", filename=data_file)\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "if data_file.endswith(\".zst\"):\n",
    "    with open(path, \"rb\") as fh:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding=\"utf-8\")\n",
    "            for line in text_stream:\n",
    "                obj = json.loads(line)\n",
    "                title = (obj.get(\"title\") or \"\").strip()\n",
    "                body = (obj.get(\"text\") or \"\").strip()\n",
    "                source = (obj.get(\"source\") or \"unknown\").strip()\n",
    "                if not body:\n",
    "                    continue\n",
    "                texts.append(f\"{title}. {body}\" if title else body)\n",
    "                labels.append(source)\n",
    "                if len(texts) >= N_DOCS:\n",
    "                    break\n",
    "else:\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for obj in reader:\n",
    "            title = (obj.get(\"title\") or \"\").strip()\n",
    "            body = (obj.get(\"text\") or \"\").strip()\n",
    "            source = (obj.get(\"source\") or \"unknown\").strip()\n",
    "            if not body:\n",
    "                continue\n",
    "            texts.append(f\"{title}. {body}\" if title else body)\n",
    "            labels.append(source)\n",
    "            if len(texts) >= N_DOCS:\n",
    "                break\n",
    "\n",
    "metadatas = [{\"label\": lbl, \"source\": \"ru_news\", \"source_id\": i} for i, lbl in enumerate(labels)]\n",
    "ids = [f\"doc-{i}\" for i in range(len(texts))]\n",
    "\n",
    "print(\"Документов:\", len(texts))\n",
    "print(\"Пример:\", texts[0][:200])\n",
    "print(\"Источник пример:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809df9a",
   "metadata": {},
   "source": [
    "### 1.3 Эмбеддинги и индексация\n",
    "\n",
    "Создаём эмбеддинги и индексируем документы в Chroma (HNSW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from pathlib import Path\n",
    "\n",
    "# Выбор бэкенда эмбеддингов\n",
    "if EMBEDDINGS_BACKEND == \"hf\":\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    embed_namespace = f\"{DATASET_MODE}_hf\"\n",
    "else:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", base_url=os.environ.get(\"OPENAI_BASE_URL\"))\n",
    "    embed_namespace = f\"{DATASET_MODE}_openai\"\n",
    "\n",
    "store = LocalFileStore(\"./cache/embeddings\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(embeddings, store, namespace=embed_namespace)\n",
    "\n",
    "persist_dir = Path(\"./db/chroma_lenta_ru\")\n",
    "\n",
    "collection_metadata = {\n",
    "    \"hnsw:space\": \"cosine\",\n",
    "    \"hnsw:M\": 16,\n",
    "    \"hnsw:construction_ef\": 200,\n",
    "    \"hnsw:search_ef\": 64,\n",
    "}\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"lenta_ru_cosine\",\n",
    "    embedding_function=cached_embedder,\n",
    "    persist_directory=str(persist_dir),\n",
    "    collection_metadata=collection_metadata,\n",
    ")\n",
    "\n",
    "vectorstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
    "\n",
    "if hasattr(vectorstore, \"persist\"):\n",
    "    vectorstore.persist()\n",
    "\n",
    "print(\"Индекс готов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd529f",
   "metadata": {},
   "source": [
    "## Часть 2. Реализация поиска\n",
    "\n",
    "### 2.1 Базовый семантический поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ea2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"экономика и рост ВВП\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nРезультат {i} (topic={doc.metadata.get('label')}):\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97247a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Фильтрация по метаданным (например, тема \"политика\")\n",
    "from collections import Counter\n",
    "\n",
    "query = \"рынок нефти и цены\"\n",
    "label_filter = \"политика\"\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "print(\"Топ меток:\", label_counts.most_common(10))\n",
    "\n",
    "# Если датасет ru_news, метки — это источники, а не темы\n",
    "if label_filter not in label_counts:\n",
    "    candidates = [l for l in label_counts if label_filter.lower() in l.lower()]\n",
    "    if candidates:\n",
    "        label_filter = candidates[0]\n",
    "        print(f\"Подобран label по подстроке: {label_filter}\")\n",
    "    else:\n",
    "        label_filter = label_counts.most_common(1)[0][0]\n",
    "        print(f\"Label 'политика' не найден. Используем: {label_filter}\")\n",
    "\n",
    "try:\n",
    "    filtered = vectorstore.similarity_search(query, k=3, filter={\"label\": label_filter})\n",
    "except Exception:\n",
    "    base = vectorstore.similarity_search(query, k=10)\n",
    "    filtered = [d for d in base if d.metadata.get(\"label\") == label_filter][:3]\n",
    "\n",
    "for i, doc in enumerate(filtered, 1):\n",
    "    print(f\"\\nФильтрованный результат {i} (topic={doc.metadata.get('label')}):\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Trade‑offs: скорость vs точность\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "emb_matrix = np.array(embeddings.embed_documents(texts))\n",
    "queries = [\n",
    "    \"международные отношения и конфликт\",\n",
    "    \"футбольный матч и победа\",\n",
    "    \"финансовый рынок и прибыль\",\n",
    "    \"новая технология и наука\",\n",
    "]\n",
    "\n",
    "k = 5\n",
    "\n",
    "emb_matrix_norm = emb_matrix / np.linalg.norm(emb_matrix, axis=1, keepdims=True)\n",
    "\n",
    "exact_ids = []\n",
    "start = time.perf_counter()\n",
    "for q in queries:\n",
    "    q_emb = np.array(embeddings.embed_query(q))\n",
    "    q_emb = q_emb / np.linalg.norm(q_emb)\n",
    "    sims = emb_matrix_norm @ q_emb\n",
    "    topk = np.argsort(-sims)[:k]\n",
    "    exact_ids.append(set(topk.tolist()))\n",
    "exact_time = time.perf_counter() - start\n",
    "\n",
    "ann_ids = []\n",
    "start = time.perf_counter()\n",
    "for q in queries:\n",
    "    docs = vectorstore.similarity_search(q, k=k)\n",
    "    ids_found = set(int(d.metadata.get(\"source_id\")) for d in docs)\n",
    "    ann_ids.append(ids_found)\n",
    "ann_time = time.perf_counter() - start\n",
    "\n",
    "recalls = []\n",
    "for e, a in zip(exact_ids, ann_ids):\n",
    "    recalls.append(len(e & a) / len(e))\n",
    "\n",
    "print(\"Exact time:\", round(exact_time, 3), \"s\")\n",
    "print(\"ANN time:\", round(ann_time, 3), \"s\")\n",
    "print(\"Recall@k:\", round(sum(recalls) / len(recalls), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7798dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels_plot = [\"Exact\", \"ANN\"]\n",
    "recall_plot = [1.0, sum(recalls) / len(recalls)]\n",
    "time_plot = [exact_time, ann_time]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "ax1.bar(labels_plot, time_plot, color=[\"#5DADE2\", \"#58D68D\"], alpha=0.8)\n",
    "ax1.set_ylabel(\"Time (s)\")\n",
    "ax1.set_title(\"Speed vs Recall@k (Lenta.ru)\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(labels_plot, recall_plot, color=\"#AF7AC5\", marker=\"o\")\n",
    "ax2.set_ylabel(\"Recall@k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841677a1",
   "metadata": {},
   "source": [
    "## Выводы по экспериментам\n",
    "\n",
    "- **Индексация и базовый поиск:** векторная БД успешно находит новости по смыслу даже при коротких запросах.\n",
    "- **Фильтрация по метаданным:** фильтры позволяют сужать поиск по теме/источнику; это повышает релевантность.\n",
    "- **Trade‑off точность/скорость:** ANN заметно быстрее brute‑force, но может терять часть релевантных результатов (Recall@k < 1.0).\n",
    "- **Параметр `search_ef`:** увеличение повышает Recall@k, но замедляет поиск.\n",
    "- **Гибридный поиск:** сочетание BM25 + векторного поиска часто даёт стабильнее результаты на новостных запросах.\n",
    "\n",
    "**Практический итог:** для продакшена оптимально использовать ANN с настроенными параметрами, хранить метаданные и при необходимости включать гибридный поиск."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e03194",
   "metadata": {},
   "source": [
    "## Автоматические выводы по вашим метрикам\n",
    "\n",
    "Эта ячейка сформирует итог на основе измеренных `exact_time`, `ann_time`, `recall@k` и параметров `search_ef`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f830e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(val, default=None):\n",
    "    return val if val is not None else default\n",
    "\n",
    "summary = []\n",
    "\n",
    "# Скорость vs точность\n",
    "if \"exact_time\" in globals() and \"ann_time\" in globals():\n",
    "    speedup = exact_time / ann_time if ann_time else None\n",
    "    summary.append(f\"Exact: {exact_time:.3f}s, ANN: {ann_time:.3f}s\")\n",
    "    if speedup:\n",
    "        summary.append(f\"Ускорение ANN ≈ {speedup:.2f}x\")\n",
    "\n",
    "if \"recalls\" in globals() and len(recalls) > 0:\n",
    "    summary.append(f\"Recall@k: {sum(recalls)/len(recalls):.3f}\")\n",
    "\n",
    "# search_ef сравнение\n",
    "if \"fast_recall\" in globals() and \"acc_recall\" in globals():\n",
    "    summary.append(f\"search_ef FAST recall@k: {fast_recall:.3f}\")\n",
    "    summary.append(f\"search_ef ACCUR recall@k: {acc_recall:.3f}\")\n",
    "\n",
    "print(\"\\n\".join(summary) if summary else \"Метрики не найдены — запустите ячейки 2.4 и 2.5.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
