# Модуль 4 — RAG: карта понимания

Это как найти нужную страницу в книге по оглавлению.

Здесь подробная и понятная карта модуля 4: что мы делали и зачем.

---

## 1) Что такое RAG (суть в 4 пунктах)

- У тебя есть документы.  
- У тебя есть вопрос.  
- Ретривер находит нужные кусочки.  
- LLM отвечает по найденному контексту.  

### Мини‑схема
```
Вопрос → Ретривер → Чанки → Ответ
```

### Микро‑проверка
Вопрос: кто ищет документы?  
Ответ: ретривер.

---

## 2) Где происходит прорыв

1. Вопрос → ретривер  
2. Ретривер → чанки  
3. Чанки → промпт  
4. Ответ ← ✨ **ВОТ ЗДЕСЬ МАГИЯ!**  

LLM отвечает уже с «подложенными» знаниями.

### Контраст ошибки
❌ Многие думают: «LLM сама ищет в базе»  
✅ На самом деле: ретривер ищет, LLM читает найденное  
→ Как официант приносит блюдо, а не готовит его сам.

---

## 3) Блок A — базовые кирпичики

### `00_data_preprocessing.ipynb`
**Якорь:** как сначала помыть овощи, а потом готовить.  

**Что делаем:**
- удаляем мусор  
- выравниваем текст  
- убираем дубликаты  
- сохраняем метаданные  

**Мини‑схема**
```
Сырой текст → Очистка → Готовый текст
```

**Микро‑упражнение:**  
Возьми один документ и найди 2 строки, которые точно нужно удалить.

---

### `01_load_split.ipynb`
**Якорь:** как резать хлеб на ломтики.  

**Что делаем:**
- загружаем файлы  
- режем на чанки  
- задаём размер  
- делаем перекрытие  

**Мини‑схема**
```
Документ → Чанки → Поиск
```

**Микро‑упражнение:**  
Поставь `chunk_size=300` и посмотри первый чанк.

---

### `02_embeddings_retrievers.ipynb`
**Якорь:** как дать каждому тексту координаты на карте.  

**Что делаем:**
- делаем эмбеддинги  
- кладём в базу  
- включаем кэш  
- создаём ретривер  

**Мини‑схема**
```
Текст → Вектор → База
```

**Микро‑упражнение:**  
Сравни время с кэшем и без кэша на 50 текстах.

---

### `03_hybrid_search.ipynb`
**Якорь:** как искать по ключевому слову и по смыслу вместе.  

**Что делаем:**
- включаем BM25  
- включаем вектор  
- задаём веса  
- сравниваем ответы  

**Мини‑схема**
```
BM25 + Вектор → Итог
```

**Микро‑упражнение:**  
Поменяй веса 0.8/0.2 и сравни выдачу.

---

## 4) Блок B — контекст и реальные документы

### `04_parent_retriever.ipynb`
**Якорь:** как читать не одну строку, а целый абзац.  

**Что делаем:**
- ищем по маленьким  
- возвращаем большие  
- сохраняем контекст  
- проверяем лимиты  

**Мини‑схема**
```
Малый чанк → Родитель → Ответ
```

**Микро‑упражнение:**  
Увеличь `parent_chunk` и сравни длину ответа.

---

### `05_rag_pipeline.ipynb`
**Якорь:** как собрать сэндвич из слоёв.  

**Что делаем:**
- загрузка  
- чанкинг  
- эмбеддинги  
- RetrievalQA  

**Мини‑схема**
```
Данные → База → Ретривер → Промпт → Ответ
```

**Микро‑упражнение:**  
Измени `k` и сравни качество ответа.

---

### `06_rag_from_md.ipynb`
**Якорь:** как читать инструкцию с заголовками.  

**Что делаем:**
- читаем `.md`  
- чистим разметку  
- сохраняем заголовки  
- делаем поиск  

**Мини‑схема**
```
.md → Чистка → Чанки
```

**Микро‑упражнение:**  
Добавь заголовок и проверь, пришёл ли он в метаданных.

---

### `07_rag_real_md.ipynb`
**Якорь:** как сохранить важные куски кода в инструкции.  

**Что делаем:**
- делим по заголовкам  
- не трогаем код  
- сохраняем секции  
- отвечаем по секциям  

**Мини‑схема**
```
Заголовки → Секции → Ответ
```

**Микро‑упражнение:**  
Найди секцию с кодом и проверь, что код не потерялся.

---

## 5) Блок C — ДЗ и эксперименты

### `08_hw_vector_db.ipynb`
**Якорь:** как сравнить бегуна и велосипедиста.  

**Что делаем:**
- индексируем  
- сравниваем ANN и exact  
- меняем `ef`  
- строим график  

**Мини‑схема**
```
Exact vs ANN → Recall/Time
```

**Микро‑упражнение:**  
Измени `search_ef` и посмотри, как меняется Recall@k.

---

### `09_hw_vector_db_lenta.ipynb`
**Якорь:** как тренироваться на родном языке.  

**Что делаем:**
- берём ru_news или Lenta  
- используем локальные эмбеддинги  
- фильтруем по меткам  
- делаем выводы  

**Мини‑схема**
```
RU датасет → Эмбеддинги → Поиск
```

**Микро‑упражнение:**  
Выведи топ‑5 меток и выбери одну для фильтра.

---

### `99_module4_step_by_step.ipynb`
**Якорь:** как пройти по инструкции шаг за шагом.  

**Что делаем:**
- читаем с нуля  
- повторяем всё  
- закрепляем  
- получаем уверенность  

**Мини‑схема**
```
Шаги → Практика → Понимание
```

**Микро‑упражнение:**  
Пройди первые 3 шага и запиши, где было сложнее всего.

---

## 6) Итог модуля (4 результата)

- умеешь готовить данные  
- строишь эмбеддинги и ретривер  
- собираешь полный RAG  
- работаешь с EN и RU датасетами  

---

Попробуй: открой `99_module4_step_by_step.ipynb` и пройди его сверху вниз.

---

## 7) Дополнительные код‑примеры (ещё подробнее)

Представь, как собирать конструктор из маленьких блоков.

### Пример 1: Чанкинг (разрезаем текст)
```python
text = "Очень длинный документ..."
splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
)
chunks = splitter.split_text(text)
```

### Пример 2: Эмбеддинги + база
```python
emb = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
db = Chroma.from_texts(
    texts=chunks,
    embedding=emb,
)
```

### Пример 3: Ретривер → поиск
```python
retriever = db.as_retriever(k=3)
docs = retriever.get_relevant_documents("что про экономику?")
for d in docs:
    print(d.page_content[:120])
```

### Пример 4: RAG‑промпт
```python
context = "\n\n".join([d.page_content for d in docs])
prompt = f"Контекст:\n{context}\n\nВопрос: что про экономику?"
answer = llm.invoke(prompt)
```

### Пример 5: Гибридный поиск (BM25 + Вектор)
```python
bm25 = BM25Retriever.from_texts(chunks)
vec = db.as_retriever(k=3)
hybrid = EnsembleRetriever(
    retrievers=[bm25, vec],
    weights=[0.5, 0.5],
)
```

### Пример 6: ParentDocumentRetriever
```python
parent = ParentDocumentRetriever(
    vectorstore=db,
    docstore=InMemoryStore(),
    child_splitter=RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20),
    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100),
)
```

Микро‑проверка: какой блок ты запустишь первым — чанкинг или эмбеддинги?  
Ответ: сначала чанкинг, потом эмбеддинги.