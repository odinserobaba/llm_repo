# Модуль 4 — RAG: подробный разбор

Этот файл — подробная теоретическая и практическая карта модуля 4.  
Он объясняет **зачем** нужен каждый блокнот, **как** работает ключевая техника и **как применять** её в реальных задачах.

---

## 1) `01_load_split.ipynb`

### Теория
**RAG (Retrieval‑Augmented Generation)** — подход, где модель отвечает, опираясь на внешний контент.  
Первый шаг — **загрузка** документов и **чанкинг** (разделение на фрагменты).

**Почему это важно:**  
Модель не может «помнить» все документы. Мы готовим небольшой контекст, который можно быстро найти и подать в LLM.

**Исторически:**  
Чанкинг стал базовой техникой, когда стало понятно: качество RAG сильно зависит от качества разбиения текста.

### Практика в блокноте
Вы загружаете файлы из папки `./data` и нарезаете их через `RecursiveCharacterTextSplitter`.

**Ключевые параметры:**
- `chunk_size` — размер чанка;
- `chunk_overlap` — перекрытие, чтобы не терять смысл на границах.

### Практические рекомендации
- Начинайте с 500/50.  
- Увеличивайте чанк для «плотных» документов (юридические, технические).  
- Слишком мелкие чанки ухудшают качество поиска.

---

## 2) `02_embeddings_retrievers.ipynb`

### Теория
**Эмбеддинги** превращают текст в вектор, по которому можно искать похожие фрагменты.  
**Ретривер** — компонент, который находит релевантные чанки для запроса.

**Кэширование эмбеддингов** экономит деньги и ускоряет повторные эксперименты.

### Практика в блокноте
Вы создаёте:
- `CacheBackedEmbeddings` для кэша;
- `Chroma` как векторное хранилище;
- `retriever` для поиска релевантных чанков.

### Практические рекомендации
- Всегда включайте кэш эмбеддингов.
- Храните `persist_directory`, чтобы не пересоздавать базу.
- Подбирайте `k` под свой домен (обычно 3–5).

---

## 3) `03_hybrid_search.ipynb`

### Теория
**Гибридный поиск** объединяет:
- **BM25** (ключевые слова);
- **векторный поиск** (семантика).

Это дает баланс между точностью и смыслом.

**Исторически:**  
Чистый векторный поиск часто пропускает точные термины, а BM25 не понимает смысл.  
Гибридный подход стал индустриальным стандартом.

### Практика в блокноте
Вы строите:
- `BM25Retriever`;
- `Chroma` ретривер;
- `EnsembleRetriever` с весами.

### Практические рекомендации
- Начните с весов 0.5/0.5 и подбирайте под домен.
- Если важны точные термины — увеличивайте вес BM25.
- Если важен контекст — увеличивайте вес векторного поиска.

---

## 4) `04_parent_retriever.ipynb`

### Теория
**ParentDocumentRetriever** индексирует маленькие чанки, но возвращает большие «родительские» документы.  
Это сохраняет контекст и уменьшает риск обрывочных ответов.

### Практика в блокноте
Вы настраиваете:
- `child_splitter` для поиска;
- `parent_splitter` для контекста;
- отдельное хранилище родительских документов.

### Практические рекомендации
- `child_chunk` 200–400, `parent_chunk` 800–1500 — хороший старт.
- Если ответ слишком короткий — увеличьте `parent_chunk`.
- Следите за размером итогового контекста, чтобы не переполнить лимит модели.

---

## 5) `05_rag_pipeline.ipynb`

### Теория (как работает «полный» RAG)
RAG‑ответ строится в несколько этапов. Важно понимать, **как именно LLM получает информацию**:
1. **Вопрос пользователя** поступает в ретривер.
2. **Ретривер** находит релевантные чанки в векторной базе.
3. Эти чанки собираются в **контекст**.
4. **LLM получает промпт**, в который уже встроен контекст, и формирует ответ.

То есть LLM **не ищет в базе сама** — она получает уже найденные фрагменты и отвечает на их основе.

### Практика в блокноте
Вы собираете рабочий пайплайн:
- **Загрузка документов** → чанкинг  
- **Эмбеддинги** → векторная база (Chroma)  
- **Retriever** → получает топ‑чанки  
- **RetrievalQA chain** → формирует промпт с контекстом и даёт финальный ответ

**Что делает RetrievalQA внутри:**
- берёт вопрос;
- вызывает ретривер;
- вставляет найденные чанки в шаблон;
- отправляет это в LLM;
- возвращает текст ответа.

### Как именно LLM получает контекст
LLM получает **готовую строку** с разделами примерно такого вида:
```
Контекст:
[chunk 1]
[chunk 2]
[chunk 3]

Вопрос: ...
Ответ:
```
То есть «знание» приходит не из самой модели, а из переданного контекста.

### Практические рекомендации
- Если ответ «плывёт», снижайте `temperature` и уменьшайте `k`.  
- Если модель не находит нужный факт — увеличьте `k` или улучшите чанкинг.  
- При коротких документах можно уменьшить `chunk_size` для точности.  
- Всегда логируйте **вопрос → найденные чанки → ответ** для оценки качества.

---

## Как использовать модуль 4

1. `01_load_split.ipynb` — подготовка документов.  
2. `02_embeddings_retrievers.ipynb` — векторный поиск.  
3. `03_hybrid_search.ipynb` — гибридный подход.  
4. `04_parent_retriever.ipynb` — сохранение контекста.
5. `05_rag_pipeline.ipynb` — полный рабочий RAG.

---

## Итог модуля 4

К концу модуля вы:
- умеете готовить документы для RAG;
- строите эмбеддинги и ретриверы;
- комбинируете BM25 и векторный поиск;
- сохраняете контекст через ParentDocumentRetriever;
- собираете полный RAG‑пайплайн и понимаете, как LLM получает контекст.
