{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab: Загрузка и переключение LoRA-адаптеров\n",
    "\n",
    "**Цель:** загрузить базовую модель и несколько адаптеров, переключаться между ними при генерации.\n",
    "\n",
    "> Пути к адаптерам укажите свои (Drive, локальный output, Hugging Face Hub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Установка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подключение Google Drive (опционально)\n",
    "\n",
    "Если адаптеры сохранены в Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Пример: адаптеры в Drive\n",
    "ADAPTER_JSON = \"/content/drive/MyDrive/adapters/lora_json\"\n",
    "ADAPTER_CODING = \"/content/drive/MyDrive/adapters/lora_coding\"\n",
    "ADAPTER_SUPPORT = \"/content/drive/MyDrive/adapters/lora_support\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузка модели и адаптеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Пути: подставьте свои. Для одного адаптера из peft_tools_hw — outputs_finetome/final\n",
    "ADAPTER_PATHS = {\n",
    "    \"default\": \"outputs_finetome/final\",  # после peft_tools_hw.ipynb\n",
    "    # \"json\": \"/content/drive/MyDrive/adapters/lora_json\",\n",
    "    # \"coding\": \"/content/drive/MyDrive/adapters/lora_coding\",\n",
    "    # \"support\": \"/content/drive/MyDrive/adapters/lora_support\",\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Загружаем первый адаптер (обязательно)\n",
    "first_adapter = list(ADAPTER_PATHS.keys())[0]\n",
    "first_path = ADAPTER_PATHS[first_adapter]\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, first_path, adapter_name=first_adapter)\n",
    "\n",
    "# Добавляем остальные адаптеры (если есть)\n",
    "for name, path in ADAPTER_PATHS.items():\n",
    "    if name != first_adapter:\n",
    "        try:\n",
    "            model.load_adapter(path, adapter_name=name)\n",
    "            print(f\"Загружен адаптер: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Пропущен {name}: {e}\")\n",
    "\n",
    "print(\"Доступные адаптеры:\", model.peft_config.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Генерация с переключением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, adapter_name: str = None, max_new_tokens: int = 150):\n",
    "    \"\"\"Генерация с выбранным адаптером.\"\"\"\n",
    "    if adapter_name and adapter_name in model.peft_config:\n",
    "        model.set_adapter(adapter_name)\n",
    "        print(f\"Адаптер: {adapter_name}\")\n",
    "    \n",
    "    full_prompt = f\"\"\"### Instruction:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример с одним адаптером (default)\n",
    "prompt = \"Объясни, что такое рекурсия в Python, в двух предложениях.\"\n",
    "print(generate(prompt, adapter_name=\"default\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если загружены несколько адаптеров — переключаем\n",
    "# print(generate(\"Извлеки данные в JSON\", adapter_name=\"json\"))\n",
    "# print(generate(\"Напиши функцию сортировки\", adapter_name=\"coding\"))\n",
    "# print(generate(\"Пользователь не может войти в аккаунт\", adapter_name=\"support\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Альтернатива: один адаптер через AutoPeftModelForCausalLM\n",
    "\n",
    "Если нужен только один адаптер — проще загрузить сразу base+adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import AutoPeftModelForCausalLM\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     \"outputs_finetome/final\",\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"outputs_finetome/final\")\n",
    "# # Дальше model.generate(...) как обычно"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
