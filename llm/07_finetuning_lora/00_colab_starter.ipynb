{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ –°—Ç–∞—Ä—Ç–æ–≤—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è Colab ‚Äî Fine-tuning (LoRA/PEFT)\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –Ω–æ–≤–æ–≥–æ Colab-–Ω–æ—É—Ç–±—É–∫–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –Ø—á–µ–π–∫–∞ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –Ø—á–µ–π–∫–∞ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {name}\")\n",
    "    print(f\"–ü–∞–º—è—Ç—å: {mem:.2f} GB\")\n",
    "    print(\"\\n‚Üí –ì–æ—Ç–æ–≤ –∫ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥—É!\" if mem >= 12 else \"‚Üí –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è 7B+\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω. Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –î–∞–ª–µ–µ: —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∫–æ–¥ –∏–∑ —É—Ä–æ–∫–æ–≤ 1-7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
