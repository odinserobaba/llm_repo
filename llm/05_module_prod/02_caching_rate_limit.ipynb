{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Модуль 5.2 — Кэширование и Rate Limit\n",
        "\n",
        "**Цель:** сделать вызовы LLM дешевле и стабильнее: кэшировать ответы и ограничивать частоту запросов.\n",
        "\n",
        "**Что сделаем:**\n",
        "- включим кэш в памяти и на диске\n",
        "- увидим ускорение на повторном запросе\n",
        "- добавим простое ограничение частоты (rate limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install -U \\\n",
        "  langchain \\\n",
        "  langchain-openai \\\n",
        "  langchain-community \\\n",
        "  python-dotenv \\\n",
        "  pydantic==2.12.3 \\\n",
        "  requests==2.32.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Настройка провайдера (AITunnel)\n",
        "\n",
        "Укажем ключ и базовый URL для вызовов модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Введите OPENAI_API_KEY: \")\n",
        "\n",
        "if not os.environ.get(\"OPENAI_BASE_URL\"):\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = \"https://api.aitunnel.ru/v1/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Кэш в памяти\n",
        "\n",
        "Повторный запрос с тем же промптом вернётся мгновенно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.globals import set_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-5.2-chat\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=128,\n",
        "    base_url=os.environ.get(\"OPENAI_BASE_URL\"),\n",
        ")\n",
        "\n",
        "prompt = \"Объясни одним предложением, что такое кэш.\" \n",
        "\n",
        "start = time.perf_counter()\n",
        "print(llm.invoke(prompt).content)\n",
        "print(\"Первый вызов:\", round(time.perf_counter() - start, 3), \"s\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "print(llm.invoke(prompt).content)\n",
        "print(\"Второй вызов (из кэша):\", round(time.perf_counter() - start, 3), \"s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Кэш на диске (между запусками)\n",
        "\n",
        "Если ноутбук перезапустить, кэш останется."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "\n",
        "set_llm_cache(SQLiteCache(database_path=\"./cache/langchain.sqlite\"))\n",
        "\n",
        "start = time.perf_counter()\n",
        "print(llm.invoke(prompt).content)\n",
        "print(\"Вызов с дисковым кэшем:\", round(time.perf_counter() - start, 3), \"s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Rate limit (простая защита)\n",
        "\n",
        "Ограничим частоту вызовов: не чаще 1 запроса в 2 секунды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "last_call = 0.0\n",
        "min_interval_sec = 2.0\n",
        "\n",
        "\n",
        "def rate_limited_invoke(prompt_text: str):\n",
        "    global last_call\n",
        "    now = time.time()\n",
        "    wait = min_interval_sec - (now - last_call)\n",
        "    if wait > 0:\n",
        "        time.sleep(wait)\n",
        "    last_call = time.time()\n",
        "    return llm.invoke(prompt_text).content\n",
        "\n",
        "for i in range(3):\n",
        "    t0 = time.perf_counter()\n",
        "    print(rate_limited_invoke(f\"Сообщение {i+1}\"))\n",
        "    print(\"Время:\", round(time.perf_counter() - t0, 2), \"s\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
