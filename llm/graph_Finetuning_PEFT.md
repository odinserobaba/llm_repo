## Graph: Finetuning, LoRA и PEFT

Хаб по дообучению LLM, LoRA, адаптерам и экспериментам из HW.

---

## Модули и обзор

- [[07_finetuning_lora/README|Модуль 7 — Finetuning и LoRA]]
- [[08_hw_peft_tools/README|HW — PEFT & Adapters]]

---

## Основные ноутбуки

- [[07_finetuning_lora/01_intro_peft|Введение в PEFT]]
- [[07_finetuning_lora/02_lora_basics|LoRA basics: идея и базовый пример]]
- [[07_finetuning_lora/03_classifier_peft|Классификатор с PEFT]]
- [[07_finetuning_lora/04_llm_finetune|Finetune LLM (основной пример)]]
- [[07_finetuning_lora/05_advanced_peft|Advanced PEFT — продвинутые техники]]
- [[07_finetuning_lora/06_colab_optimization|Оптимизация Colab при обучении]]
- [[07_finetuning_lora/07_deploy_inference|Deploy & inference дообученной модели]]

---

## HW по PEFT и адаптерам

- [[08_hw_peft_tools/peft_tools_hw|Основной ноутбук HW по PEFT]]
- [[08_hw_peft_tools/peft_tools_hw_result.ipynb|Результаты HW (основной)]]
- [[08_hw_peft_tools/peft_tools_hw_1500_rez.ipynb|Эксперимент с 1500 шагами]]
- [[08_hw_peft_tools/peft_tools_hw_res_small_model.ipynb|Эксперимент с маленькой моделью]]
- [[08_hw_peft_tools/peft_tools_hw_result_02_16.ipynb|Эксперименты от 02.16]]
- [[08_hw_peft_tools/adapters_colab/01_load_switch_adapters|Загрузка и переключение адаптеров]]
- [[08_hw_peft_tools/adapters_colab/02_merge_adapter|Слияние адаптеров]]
- [[08_hw_peft_tools/adapters_colab/03_router_demo|Router demo для адаптеров]]

---

## Промптинг и эксперименты

- [[08_hw_peft_tools/prompt_cognitive_designer|Промпты для HW по PEFT]]
- [[graph_Prompting|Graph: Промптинг и паттерны]] — общие техники промптинга

---

## Связи с продом и деплоем

- [[05_module_prod/README|Модуль 5 — Прод: тесты, деплой, надёжность]]
- [[07_finetuning_lora/07_deploy_inference|Деплой дообученной модели]]
- [[graph_Prod_Observability|Graph: Прод, мониторинг, Langfuse]]
- [[00_Home|← Назад в Home]]

---

**Теги:** #finetuning #lora #peft #adapters #эксперименты

