{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd8d602",
   "metadata": {},
   "source": [
    "# Модуль 1.4 — Локальные модели в Colab\n",
    "\n",
    "**Цель:** запустить локальную LLM на GPU без внешнего API и понять ограничения.\n",
    "\n",
    "**Что сделаем:**\n",
    "- проверим доступность GPU\n",
    "- установим `transformers`\n",
    "- запустим небольшую русскоязычную модель\n",
    "- разберем, когда локальные модели выгодны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d123b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3277170",
   "metadata": {},
   "source": [
    "## Проверка GPU\n",
    "\n",
    "Colab с GPU ускоряет инференс. Если GPU недоступен, все будет работать, но медленнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a3a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA доступна:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382be0b",
   "metadata": {},
   "source": [
    "## Загрузка локальной модели\n",
    "\n",
    "Для обучения берем небольшую русскоязычную модель. Она быстрее скачивается и подходит для демонстраций.\n",
    "\n",
    "**Почему не Llama‑3 в Colab:**\n",
    "- крупные модели требуют много VRAM и времени загрузки\n",
    "- в Colab часто удобнее начинать с компактных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "local_model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "prompt = \"Сгенерируй 3 варианта заголовка статьи про практическое применение LangChain.\"\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d78a51",
   "metadata": {},
   "source": [
    "## Практические рекомендации\n",
    "\n",
    "- Для демо берите компактные модели (до 1–3B), чтобы не упираться в память.\n",
    "- Снижайте `temperature`, если нужно стабильнее следовать формату.\n",
    "- Для ускорения используйте GPU и `device_map=\"auto\"`.\n",
    "- Локальные модели хороши для офлайн‑экспериментов и приватных данных.\n",
    "- Для продакшена обычно нужен более мощный сервер или оптимизация (квантование)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
