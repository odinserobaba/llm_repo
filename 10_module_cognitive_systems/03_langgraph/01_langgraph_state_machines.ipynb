{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph: Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ°Ñ State Machine Ñ RAG\n",
        "\n",
        "**Ğ¦ĞµĞ»ÑŒ:** Ğ³Ñ€Ğ°Ñ„ Ñ 6 ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸, Ğ´Ğ²ÑƒĞ¼Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ñ‘Ğ±Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ²ÑƒĞ¼Ñ Ñ†Ğ¸ĞºĞ»Ğ°Ğ¼Ğ¸ â€” Ğ²Ğ¸Ğ´Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ°.\n",
        "\n",
        "**Ğ“Ñ€Ğ°Ñ„:**\n",
        "```\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  router_ctx: ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹?      â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "START â†’ retrieve â”€â”€â”¬â†’ enrich â†’ retrieve (Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ)\n",
        "                   â””â†’ generate\n",
        "                         â”‚\n",
        "                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€ router_quality: Ğ¾Ñ‚Ğ²ĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹?\n",
        "                    â†“         â†“\n",
        "                 critique   END\n",
        "                    â”‚\n",
        "                  refine â†’ generate (Ñ†Ğ¸ĞºĞ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ Ğ¯ĞºĞ¾Ñ€ÑŒ\n",
        "Ğ”Ğ²Ğ° Ñ†Ğ¸ĞºĞ»Ğ°, Ğ´Ğ²Ğ° Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ°: `retrieveâ†”enrich` (ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚) Ğ¸ `generateâ†”critiqueâ†”refine` (ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°)."
      ],
      "id": "038bb415"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# â† 1. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° (Colab: Runtime â†’ GPU)\n",
        "!pip install -q langgraph langchain langchain-core langchain-community\n",
        "!pip install -q transformers accelerate torch"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "12be89d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> âš ï¸ **Ğ•ÑĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° `torchvision::nms does not exist`:** Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ `model.generate()` Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ pipeline â€” ÑĞ¼. [ie_extraction_hw](09_hw_ie_extraction) ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ pipeline."
      ],
      "id": "bab29c4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ°Ğ»Ğ¾Ğ¹ LLM (TinyLlama 1.1B)\n",
        "\n",
        "TinyLlama Ğ¿Ğ¾Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ² Colab T4 (~2â€“4 GB VRAM). Ğ‘ĞµĞ· 4-bit â€” Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾."
      ],
      "id": "50888bae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ, ~2.2 GB\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(\"LLM Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°, device:\", next(model.parameters()).device)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9a81dfd6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. State Ğ¸ ÑƒĞ·Ğ»Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ°\n",
        "\n",
        "- **State:** messages, context, enrich_count, critique_count, feedback\n",
        "- **retrieve:** Ğ¿Ğ¾Ğ´Ñ‚ÑĞ³Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ enrich_count)\n",
        "- **enrich:** ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ enrich_count, Ñ†Ğ¸ĞºĞ» Ğ² retrieve\n",
        "- **generate:** LLM Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ (Ğ¸Ğ»Ğ¸ Ñ feedback Ğ¸Ğ· critique)\n",
        "- **critique:** Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚, ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ feedback\n",
        "- **refine:** Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ generate Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ feedback"
      ],
      "id": "fee28c27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import TypedDict\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# â† 1. Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ°\n",
        "class State(TypedDict):\n",
        "    messages: list\n",
        "    context: str\n",
        "    enrich_count: int\n",
        "    critique_count: int\n",
        "    feedback: str\n",
        "\n",
        "# Ğ”ĞµĞ¼Ğ¾: \"Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹\" Ğ½Ğ°Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ enrich (ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ñ‹Ğ¹ retrieve)\n",
        "CHUNK = \"RAG â€” Retrieval-Augmented Generation. ĞŸĞ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² + LLM Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ.\"\n",
        "FULL_DOCS = CHUNK + \" \" + \"Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚. LLM Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.\"\n",
        "\n",
        "def retrieve(state: State) -> dict:\n",
        "    \"\"\"Ğ£Ğ·ĞµĞ» 1: Ğ¿Ğ¾Ğ´Ñ‚ÑĞ³Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ”Ğ»Ğ¸Ğ½Ğ° Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ñ enrich_count.\"\"\"\n",
        "    ec = state.get(\"enrich_count\", 0)\n",
        "    if TRACE:\n",
        "        src = \"START\" if ec == 0 else \"enrich\"\n",
        "        print(f\"\\n  [Ğ£Ğ—Ğ•Ğ›] retrieve â† Ğ¿Ğ¾ Ñ€ĞµĞ±Ñ€Ñƒ {src}â†’retrieve\")\n",
        "    ctx = CHUNK if ec == 0 else FULL_DOCS + \" [Ğ´Ğ¾Ğ¿. Ñ€Ğ°ÑƒĞ½Ğ´ \" + str(ec) + \"]\"\n",
        "    return {\"context\": ctx}\n",
        "\n",
        "def enrich(state: State) -> dict:\n",
        "    \"\"\"Ğ£Ğ·ĞµĞ» 2: Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµĞ¼ â€” ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸Ğº, Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ğ¼ ÑĞ½Ğ¾Ğ²Ğ° Ğ² retrieve.\"\"\"\n",
        "    ec = state.get(\"enrich_count\", 0) + 1\n",
        "    if TRACE:\n",
        "        print(f\"\\n  [Ğ£Ğ—Ğ•Ğ›] enrich â† Ğ¿Ğ¾ Ñ€ĞµĞ±Ñ€Ñƒ router_ctx(enrich)â†’enrich\")\n",
        "    return {\"enrich_count\": ec}\n",
        "\n",
        "def generate(state: State) -> dict:\n",
        "    \"\"\"Ğ£Ğ·ĞµĞ» 3: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚. Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ feedback â€” ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸ refine.\"\"\"\n",
        "    src = \"refineâ†’generate\" if state.get(\"feedback\") else \"retrieveâ†’generate\"\n",
        "    if TRACE:\n",
        "        print(f\"\\n  [Ğ£Ğ—Ğ•Ğ›] generate â† Ğ¿Ğ¾ Ñ€ĞµĞ±Ñ€Ñƒ {src} (Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğµ)\")\n",
        "    question = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
        "    fb = state.get(\"feedback\", \"\")\n",
        "    extra = f\"\\nĞ£Ñ‡Ñ‚Ğ¸ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ: {fb}\" if fb else \"\"\n",
        "    prompt = f\"\"\"ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {state['context']}\\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}{extra}\\nĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚:\"\"\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response)], \"feedback\": \"\"}\n",
        "\n",
        "def critique(state: State) -> dict:\n",
        "    \"\"\"Ğ£Ğ·ĞµĞ» 4: ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒĞµĞ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ·Ğ°Ğ´Ğ°Ñ‘Ğ¼ feedback Ğ´Ğ»Ñ refine.\"\"\"\n",
        "    if TRACE:\n",
        "        print(f\"\\n  [Ğ£Ğ—Ğ•Ğ›] critique â† Ğ¿Ğ¾ Ñ€ĞµĞ±Ñ€Ñƒ router_quality(critique)â†’critique\")\n",
        "    last = state[\"messages\"][-1]\n",
        "    text = last.content if hasattr(last, \"content\") else str(last)\n",
        "    return {\n",
        "        \"feedback\": \"Ğ”Ğ°Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.\",\n",
        "        \"critique_count\": state.get(\"critique_count\", 0) + 1,\n",
        "    }\n",
        "\n",
        "def refine(state: State) -> dict:\n",
        "    \"\"\"Ğ£Ğ·ĞµĞ» 5: Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² generate Ñ feedback (ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ÑƒĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ critique).\"\"\"\n",
        "    if TRACE:\n",
        "        print(f\"\\n  [Ğ£Ğ—Ğ•Ğ›] refine â† Ğ¿Ğ¾ Ñ€ĞµĞ±Ñ€Ñƒ critiqueâ†’refine (Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğµ)\")\n",
        "    return {}  # generate Ğ²Ğ¾Ğ·ÑŒĞ¼Ñ‘Ñ‚ feedback Ğ¸Ğ· state\n",
        "\n",
        "# Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° (False = Ğ±ĞµĞ· Ğ»Ğ¾Ğ³Ğ¾Ğ²)\n",
        "TRACE = True"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d8889738"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ğ Ğ¾ÑƒÑ‚ĞµÑ€Ñ‹: ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ€Ñ‘Ğ±Ñ€Ğ°\n",
        "\n",
        "- **router_ctx** (Ğ¿Ğ¾ÑĞ»Ğµ retrieve): ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ (< 100 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²) Ğ¸ enrich_count < 2 â†’ enrich, Ğ¸Ğ½Ğ°Ñ‡Ğµ â†’ generate\n",
        "- **router_quality** (Ğ¿Ğ¾ÑĞ»Ğµ generate): Ğ¾Ñ‚Ğ²ĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ (< 30 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²) Ğ¸ critique_count < 2 â†’ critique, Ğ¸Ğ½Ğ°Ñ‡Ğµ â†’ END"
      ],
      "id": "9cdf446c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def router_ctx(state: State) -> str:\n",
        "    \"\"\"Ğ Ğ¾ÑƒÑ‚ĞµÑ€ 1: ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹? â†’ enrich (Ñ†Ğ¸ĞºĞ»), Ğ¸Ğ½Ğ°Ñ‡Ğµ â†’ generate.\"\"\"\n",
        "    ctx = state.get(\"context\", \"\")\n",
        "    ec = state.get(\"enrich_count\", 0)\n",
        "    short_ctx = len(ctx.strip()) < 100\n",
        "    if short_ctx and ec < 2:\n",
        "        if TRACE:\n",
        "            print(f\"  [Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•] router_ctx: context={len(ctx)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², enrich_count={ec} â†’ enrich\")\n",
        "        return \"enrich\"\n",
        "    if TRACE:\n",
        "        print(f\"  [Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•] router_ctx: context={len(ctx)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² â†’ generate\")\n",
        "    return \"generate\"\n",
        "\n",
        "def router_quality(state: State) -> str:\n",
        "    \"\"\"Ğ Ğ¾ÑƒÑ‚ĞµÑ€ 2: Ğ¾Ñ‚Ğ²ĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹? â†’ critique (Ñ†Ğ¸ĞºĞ» refine), Ğ¸Ğ½Ğ°Ñ‡Ğµ â†’ end.\"\"\"\n",
        "    last = state[\"messages\"][-1] if state.get(\"messages\") else None\n",
        "    text = (last.content if hasattr(last, \"content\") else str(last)) if last else \"\"\n",
        "    cc = state.get(\"critique_count\", 0)\n",
        "    short = len(text.strip()) < 30\n",
        "    if short and cc < 2:\n",
        "        if TRACE:\n",
        "            print(f\"  [Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•] router_quality: Ğ¾Ñ‚Ğ²ĞµÑ‚={len(text.strip())} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², critique_count={cc} â†’ critique\")\n",
        "        return \"critique\"\n",
        "    if TRACE:\n",
        "        print(f\"  [Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•] router_quality: Ğ¾Ñ‚Ğ²ĞµÑ‚={len(text.strip())} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² â†’ end\")\n",
        "    return \"end\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ca56ac84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° Ğ³Ñ€Ğ°Ñ„Ğ°: ÑƒĞ·Ğ»Ñ‹ + Ñ€Ñ‘Ğ±Ñ€Ğ°\n",
        "\n",
        "- Ğ–Ñ‘ÑÑ‚ĞºĞ¸Ğµ: `enrichâ†’retrieve`, `critiqueâ†’refine`, `refineâ†’generate`\n",
        "- Ğ£ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ: `retrieveâ†’[router_ctx]â†’enrich|generate`, `generateâ†’[router_quality]â†’critique|END`"
      ],
      "id": "90d0026d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Ğ£Ğ·Ğ»Ñ‹\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"enrich\", enrich)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.add_node(\"critique\", critique)\n",
        "workflow.add_node(\"refine\", refine)\n",
        "\n",
        "# Ğ¡Ñ‚Ğ°Ñ€Ñ‚\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# Ğ£ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ±Ñ€Ğ¾ 1: retrieve â†’ [router_ctx] â†’ enrich | generate\n",
        "workflow.add_conditional_edges(\"retrieve\", router_ctx, {\"enrich\": \"enrich\", \"generate\": \"generate\"})\n",
        "\n",
        "# Ğ–Ñ‘ÑÑ‚ĞºĞ¾Ğµ: enrich â†’ retrieve (Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°)\n",
        "workflow.add_edge(\"enrich\", \"retrieve\")\n",
        "\n",
        "# Ğ£ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ±Ñ€Ğ¾ 2: generate â†’ [router_quality] â†’ critique | END\n",
        "workflow.add_conditional_edges(\"generate\", router_quality, {\"critique\": \"critique\", \"end\": END})\n",
        "\n",
        "# Ğ–Ñ‘ÑÑ‚ĞºĞ¸Ğµ: critique â†’ refine â†’ generate (Ñ†Ğ¸ĞºĞ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°)\n",
        "workflow.add_edge(\"critique\", \"refine\")\n",
        "workflow.add_edge(\"refine\", \"generate\")\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"Ğ“Ñ€Ğ°Ñ„ ÑĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½: 6 ÑƒĞ·Ğ»Ğ¾Ğ², 2 ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ€Ñ‘Ğ±Ñ€Ğ°, 2 Ñ†Ğ¸ĞºĞ»Ğ° (enrichâ†”retrieve, critiqueâ†’refineâ†’generate)\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "377387a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ğ¢Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ°: ĞºĞ°Ğº Ğ³Ñ€Ğ°Ñ„ Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ñ€Ñ‘Ğ±Ñ€Ğ°Ğ¼\n",
        "\n",
        "ĞŸÑ€Ğ¸ `invoke()` Ğ±ÑƒĞ´ĞµÑ‚ Ğ²Ğ¸Ğ´Ğ½Ğ¾:\n",
        "- **Ğ£Ğ—Ğ•Ğ›** â€” Ğ²Ñ…Ğ¾Ğ´ Ğ² ÑƒĞ·ĞµĞ» Ğ¸ Ğ¿Ğ¾ ĞºĞ°ĞºĞ¾Ğ¼Ñƒ Ñ€ĞµĞ±Ñ€Ñƒ Ğ¿Ñ€Ğ¸ÑˆĞ»Ğ¸\n",
        "- **Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•** â€” Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ¾Ğ²: `router_ctx` (enrich|generate), `router_quality` (critique|end)"
      ],
      "id": "b678c9e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ğ—Ğ°Ğ¿ÑƒÑĞº"
      ],
      "id": "2466fa22"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=== Ğ¢Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° ===\")\n",
        "result = app.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ RAG?\")],\n",
        "        \"context\": \"\",\n",
        "        \"enrich_count\": 0,\n",
        "        \"critique_count\": 0,\n",
        "        \"feedback\": \"\",\n",
        "    },\n",
        "    config={\"recursion_limit\": 15},  # Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ñ†Ğ¸ĞºĞ»Ğ¾Ğ²\n",
        ")\n",
        "\n",
        "print(\"\\n=== Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ===\")\n",
        "for i, m in enumerate(result[\"messages\"]):\n",
        "    role = \"User\" if isinstance(m, HumanMessage) else \"Assistant\"\n",
        "    print(f\"[{role}] {m.content[:200]}{'...' if len(str(m.content)) > 200 else ''}\")\n",
        "\n",
        "print(\"\\nenrich_count:\", result.get(\"enrich_count\", 0), \"| critique_count:\", result.get(\"critique_count\", 0))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9acc5df0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ğ¡Ñ…ĞµĞ¼Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°\n",
        "\n",
        "```\n",
        "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "     â”‚  retrieve â”‚  â† entry\n",
        "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚ add_conditional_edges (router_ctx)\n",
        "     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
        "     â†“           â†“\n",
        " enrich      generate\n",
        "     â”‚           â”‚\n",
        "     â”‚ add_edge  â”‚ add_conditional_edges (router_quality)\n",
        "     â†“           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        " retrieve      critique   end â†’ END\n",
        " (Ñ†Ğ¸ĞºĞ»)           â”‚\n",
        "                  â”‚ add_edge\n",
        "                  â†“\n",
        "               refine\n",
        "                  â”‚ add_edge\n",
        "                  â†“\n",
        "               generate\n",
        "              (Ñ†Ğ¸ĞºĞ»)\n",
        "```"
      ],
      "id": "7493c95e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}