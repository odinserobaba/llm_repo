{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "038bb415",
      "metadata": {},
      "source": [
        "# LangGraph: State Machines Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM\n",
        "\n",
        "**Ğ¦ĞµĞ»ÑŒ:** Ğ³Ñ€Ğ°Ñ„ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ TinyLlama, Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ñ‘Ğ±Ñ€Ğ°Ğ¼Ğ¸.\n",
        "\n",
        "**Ğ“Ñ€Ğ°Ñ„:**\n",
        "```\n",
        "retrieve â†’ generate â†’ [router] â”€â”¬â†’ retry (retrieve)  [ĞµÑĞ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹]\n",
        "                                â””â†’ END               [Ğ¸Ğ½Ğ°Ñ‡Ğµ]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ Ğ¯ĞºĞ¾Ñ€ÑŒ\n",
        "LangGraph = ĞºĞ°Ñ€Ñ‚Ğ° Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¾Ğ²: Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğµ Ñ€Ñ‘Ğ±Ñ€Ğ° (Ğ²ÑĞµĞ³Ğ´Ğ° Aâ†’B) Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ (router Ñ€ĞµÑˆĞ°ĞµÑ‚: retry Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½ĞµÑ†)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12be89d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# â† 1. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° (Colab: Runtime â†’ GPU)\n",
        "!pip install -q langgraph langchain langchain-core langchain-community\n",
        "!pip install -q transformers accelerate torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab29c4e",
      "metadata": {},
      "source": [
        "> âš ï¸ **Ğ•ÑĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° `torchvision::nms does not exist`:** Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ `model.generate()` Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ pipeline â€” ÑĞ¼. [ie_extraction_hw](09_hw_ie_extraction) ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50888bae",
      "metadata": {},
      "source": [
        "## 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ°Ğ»Ğ¾Ğ¹ LLM (TinyLlama 1.1B)\n",
        "\n",
        "TinyLlama Ğ¿Ğ¾Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ² Colab T4 (~2â€“4 GB VRAM). Ğ‘ĞµĞ· 4-bit â€” Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a81dfd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ, ~2.2 GB\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(\"LLM Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°, device:\", next(model.parameters()).device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee28c27",
      "metadata": {},
      "source": [
        "## 2. State Ğ¸ ÑƒĞ·Ğ»Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ°\n",
        "\n",
        "- **State:** messages, context, retry_count\n",
        "- **retrieve:** Ğ¿Ğ¾Ğ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (Ğ´ĞµĞ¼Ğ¾ â€” Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹)\n",
        "- **generate:** Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ LLM Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8889738",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# â† 1. Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ°\n",
        "class State(TypedDict):\n",
        "    messages: list\n",
        "    context: str\n",
        "    retry_count: int\n",
        "\n",
        "# Ğ”ĞµĞ¼Ğ¾-Ğ±Ğ°Ğ·Ğ° \"Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²\" (Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ â€” retriever.invoke)\n",
        "FAKE_DOCS = \"\"\"RAG â€” ÑÑ‚Ğ¾ Retrieval-Augmented Generation. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ñ‰ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¸ Ğ¿Ğ¾Ğ´ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ LLM. LLM Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ²Ğ¾Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.\"\"\"\n",
        "\n",
        "def retrieve(state: State) -> dict:\n",
        "    \"\"\"Ğ£Ğ·ĞµĞ» 1: Â«Ğ¿Ğ¾Ğ´Ñ‚ÑĞ³Ğ¸Ğ²Ğ°ĞµĞ¼Â» ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ’ Ğ´ĞµĞ¼Ğ¾ â€” Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹.\"\"\"\n",
        "    return {\"context\": FAKE_DOCS}\n",
        "\n",
        "def generate(state: State) -> dict:\n",
        "    \"\"\"Ğ£Ğ·ĞµĞ» 2: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ.\"\"\"\n",
        "    if TRACE:\n",
        "        print(f\"\\n  [Ğ£Ğ—Ğ•Ğ›] generate â† Ğ¿Ğ¾ Ñ€ĞµĞ±Ñ€Ñƒ retrieveâ†’generate (Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğµ)\")\n",
        "    question = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
        "    prompt = f\"\"\"ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {state['context']}\\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}\\nĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚:\"\"\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response)]}\n",
        "\n",
        "# Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° (False = Ğ±ĞµĞ· Ğ»Ğ¾Ğ³Ğ¾Ğ²)\n",
        "TRACE = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cdf446c",
      "metadata": {},
      "source": [
        "## 3. Ğ Ğ¾ÑƒÑ‚ĞµÑ€: ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ±Ñ€Ğ¾\n",
        "\n",
        "Ğ•ÑĞ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ˜ retry_count < 2 â†’ Ğ¸Ğ´Ñ‘Ğ¼ ÑĞ½Ğ¾Ğ²Ğ° Ğ² retrieve (Ñ†Ğ¸ĞºĞ»). Ğ˜Ğ½Ğ°Ñ‡Ğµ â†’ END."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca56ac84",
      "metadata": {},
      "outputs": [],
      "source": [
        "def router(state: State) -> str:\n",
        "    \"\"\"Ğ Ğ¾ÑƒÑ‚ĞµÑ€: Ñ€ĞµÑˆĞ°ĞµĞ¼, retry Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½ĞµÑ†.\"\"\"\n",
        "    last = state[\"messages\"][-1]\n",
        "    text = last.content if hasattr(last, \"content\") else str(last)\n",
        "    rc = state.get(\"retry_count\", 0)\n",
        "    short = len(text.strip()) < 20\n",
        "    # ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ (< 20 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ°) â€” Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ·\n",
        "    if short and rc < 2:\n",
        "        if TRACE:\n",
        "            print(f\"  [Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•] router: Ğ¾Ñ‚Ğ²ĞµÑ‚={len(text.strip())} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², retry_count={rc} â†’ retry (Ñ†Ğ¸ĞºĞ» Ğ² retrieve)\")\n",
        "        return \"retry\"\n",
        "    if TRACE:\n",
        "        print(f\"  [Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•] router: Ğ¾Ñ‚Ğ²ĞµÑ‚={len(text.strip())} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², retry_count={rc} â†’ end (Ğ²Ñ‹Ñ…Ğ¾Ğ´)\")\n",
        "    return \"end\"\n",
        "\n",
        "#        â†‘\n",
        "#        â””â”€â”€ âœ¨ Ğ’ĞĞ¢ Ğ—Ğ”Ğ•Ğ¡Ğ¬ ĞœĞĞ“Ğ˜Ğ¯: Ğ¿Ğ¾ state Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑƒĞ·ĞµĞ»"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d0026d",
      "metadata": {},
      "source": [
        "## 4. Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° Ğ³Ñ€Ğ°Ñ„Ğ°: ÑƒĞ·Ğ»Ñ‹ + Ñ€Ñ‘Ğ±Ñ€Ğ°\n",
        "\n",
        "- Ğ–Ñ‘ÑÑ‚ĞºĞ¾Ğµ Ñ€ĞµĞ±Ñ€Ğ¾: `retrieve â†’ generate`\n",
        "- Ğ£ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ±Ñ€Ğ¾: `generate â†’ [router] â†’ retrieve | END`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377387a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# ĞŸÑ€Ğ¸ retry ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸Ğº (Ñ€Ğ¾ÑƒÑ‚ĞµÑ€ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ†Ğ¸ĞºĞ»)\n",
        "def retrieve_with_count(state: State) -> dict:\n",
        "    step = state.get(\"retry_count\", 0) + 1\n",
        "    if TRACE:\n",
        "        src = \"START\" if step == 1 else \"router(retry)\"\n",
        "        print(f\"\\n  [Ğ£Ğ—Ğ•Ğ›] retrieve â† Ğ¿Ğ¾ Ñ€ĞµĞ±Ñ€Ñƒ {src}â†’retrieve (Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ {step})\")\n",
        "    out = retrieve(state)\n",
        "    out[\"retry_count\"] = step\n",
        "    return out\n",
        "\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Ğ£Ğ·Ğ»Ñ‹\n",
        "workflow.add_node(\"retrieve\", retrieve_with_count)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "\n",
        "# Ğ–Ñ‘ÑÑ‚ĞºĞ¾Ğµ Ñ€ĞµĞ±Ñ€Ğ¾: retrieve â†’ generate\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "\n",
        "# Ğ£ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ±Ñ€Ğ¾: generate â†’ router â†’ retrieve Ğ¸Ğ»Ğ¸ END\n",
        "workflow.add_conditional_edges(\"generate\", router, {\"retry\": \"retrieve\", \"end\": END})\n",
        "\n",
        "# Ğ¡Ñ‚Ğ°Ñ€Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ·ĞµĞ»\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "app = workflow.compile()  # recursion_limit Ğ·Ğ°Ğ´Ğ°Ñ‘Ñ‚ÑÑ Ğ² config Ğ¿Ñ€Ğ¸ invoke()\n",
        "print(\"Ğ“Ñ€Ğ°Ñ„ ÑĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½. Ğ Ñ‘Ğ±Ñ€Ğ°: retrieveâ†’generate, generateâ†’[router]â†’retrieve|END\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b678c9e7",
      "metadata": {},
      "source": [
        "## 5. Ğ¢Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ°: ĞºĞ°Ğº Ğ³Ñ€Ğ°Ñ„ Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ñ€Ñ‘Ğ±Ñ€Ğ°Ğ¼\n",
        "\n",
        "ĞŸÑ€Ğ¸ `invoke()` Ğ½Ğ¸Ğ¶Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ²Ğ¸Ğ´Ğ½Ğ¾:\n",
        "- **Ğ£Ğ—Ğ•Ğ›** â€” Ğ²Ñ…Ğ¾Ğ´ Ğ² ÑƒĞ·ĞµĞ» Ğ¸ Ğ¿Ğ¾ ĞºĞ°ĞºĞ¾Ğ¼Ñƒ Ñ€ĞµĞ±Ñ€Ñƒ Ğ¿Ñ€Ğ¸ÑˆĞ»Ğ¸\n",
        "- **Ğ’Ğ•Ğ¢Ğ’Ğ›Ğ•ĞĞ˜Ğ•** â€” Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ°: `retry` (Ñ†Ğ¸ĞºĞ») Ğ¸Ğ»Ğ¸ `end` (Ğ²Ñ‹Ñ…Ğ¾Ğ´)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2466fa22",
      "metadata": {},
      "source": [
        "## 6. Ğ—Ğ°Ğ¿ÑƒÑĞº"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acc5df0",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Ğ¢Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° ===\")\n",
        "result = app.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ RAG?\")],\n",
        "        \"context\": \"\",\n",
        "        \"retry_count\": 0,\n",
        "    },\n",
        "    config={\"recursion_limit\": 5},  # Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°\n",
        ")\n",
        "\n",
        "print(\"=== Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ===\")\n",
        "for i, m in enumerate(result[\"messages\"]):\n",
        "    role = \"User\" if isinstance(m, HumanMessage) else \"Assistant\"\n",
        "    print(f\"[{role}] {m.content[:200]}{'...' if len(str(m.content)) > 200 else ''}\")\n",
        "\n",
        "print(\"\\nRetry count:\", result.get(\"retry_count\", 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7493c95e",
      "metadata": {},
      "source": [
        "## Ğ¡Ñ…ĞµĞ¼Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°\n",
        "\n",
        "```\n",
        "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "     â”‚  retrieve â”‚  â† entry\n",
        "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚ add_edge (Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğµ)\n",
        "           â†“\n",
        "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "     â”‚  generate â”‚\n",
        "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚ add_conditional_edges\n",
        "     router(state)\n",
        "           â”‚\n",
        "     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
        "     â†“           â†“\n",
        " \"retry\"       \"end\"\n",
        "     â”‚           â”‚\n",
        "     â†“           â†“\n",
        " retrieve      END\n",
        " (Ñ†Ğ¸ĞºĞ»)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
