{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 6: Оптимизация под ограничения Colab\n",
    "\n",
    "**Решение проблем бесплатного Colab:**\n",
    "\n",
    "| Проблема | Решение |\n",
    "|----------|----------|\n",
    "| Ограничение памяти (12-15GB) | 4-bit квантизация + gradient_checkpointing |\n",
    "| Таймаут сессии (12 часов) | Автосохранение чекпоинтов каждые 50 шагов |\n",
    "| Медленная загрузка | Кэш датасета в Google Drive |\n",
    "| Отключение GPU | Восстановление из чекпоинта через accelerate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class ColabSafeCallback(TrainerCallback):\n",
    "    \"\"\"Сохраняет чекпоинт каждые 50 шагов — защита от обрыва сессии Colab.\"\"\"\n",
    "    def __init__(self, save_path=\"/content/drive/MyDrive/checkpoints\"):\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 50 == 0 and state.global_step > 0:\n",
    "            model = kwargs.get(\"model\")\n",
    "            if model:\n",
    "                path = f\"{self.save_path}/step-{state.global_step}\"\n",
    "                model.save_pretrained(path)\n",
    "                print(f\"✅ Чекпоинт сохранён: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование с Trainer\n",
    "print(\"\"\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    ...\n",
    "    callbacks=[ColabSafeCallback()],\n",
    "    gradient_checkpointing=True,  # экономия ~30% памяти\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка GPU и памяти\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    mem_gb = props.total_memory / 1024**3\n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"Память: {mem_gb:.2f} GB\")\n",
    "    print(f\"Рекомендация: 4-bit для моделей 7B+ при <16GB\" if mem_gb < 16 else \"Можно full fine-tune маленьких моделей\")\n",
    "else:\n",
    "    print(\"⚠️ GPU не обнаружен. В Colab: Runtime → Change runtime type → GPU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
