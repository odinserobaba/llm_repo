# Fine-tuning (LoRA/PEFT): карта понимания

```
┌─────────────────────────────────────────────────────────────┐
│  FINE-TUNING как донастройка переводчика: не учим заново,   │
│  а даём 1000 примеров «вопрос → ответ»                      │
├────────────┬────────────┬────────────┬──────────────────────┤
│ Якорь     │ Механика   │ Прорыв     │ Применение           │
│ 3 сек     │ 20 сек     │ ✨ 5 сек   │ 10 сек               │
│ Переводчик│ LoRA 0.05%│ Адаптер    │ Colab, деплой,        │
│ + примеры │ матриц     │ 50 MB      │ merge                 │
└────────────┴────────────┴────────────┴──────────────────────┘
```

---

## 🎯 Якорь + эмоциональный мостик

**🎯 ЯКОРЬ:** Переводчик учил английский годами. Вместо обучения с нуля даём ему 1000 примеров: «когда спрашивают вот так — отвечай вот так». Сохраняет знания, но следует нашему формату.

💡 **Эмпатия:** «Звучит как переписывание мозга? LoRA трогает 0.05% весов — это настройка, а не пересборка.»

---

## 📖 Термины и понятия

| Термин | Что это | Метафора |
|--------|---------|----------|
| **Fine-tuning** | Дообучение на своих примерах. Часы, не месяцы. | ✂️ Подгонка костюма |
| **PEFT / LoRA** | Обучаем ~0.05% весов. Матрицы A,B в attention. | 🔧 Сменная насадка |
| **Адаптер** | Файл ~50 MB. Накладываем на base — меняется поведение. | 📎 Закладка-инструкция |
| **4-bit** | Веса в 4 бита. В 4× меньше памяти. Colab T4. | 🗜️ Сжатие архива |
| **SFT** | У примера есть правильный ответ. Минимизируем loss. | 📝 Решебник |
| **Merge** | Вшить адаптер в base. Один файл, без PEFT. | 🧵 Пришить заплатку |
| **Colab** | Облачный Jupyter с GPU T4. ~12 ч сессия. | ☁️ Компьютер в облаке |

---

## 🔷 Прогрессивная схема (3 фазы)

#### Фаза 1: Без LoRA vs с LoRA
```
┌──────────────┐                    ┌──────────────┐
│  Full FT     │ 7B весов, 80+ GB   │  LoRA        │ 4M весов, 16 GB
│  всё меняем  │                    │  A,B матрицы │
└──────────────┘                    └──────┬───────┘
                                           ↓
                                    ┌──────────────┐
                                    │  Адаптер     │ ~50 MB
                                    │  накладываем │
                                    └──────────────┘
```

#### Фаза 2: Обучение
```
┌──────────────┐     ┌──────────────┐
│ Instruction  │ ─→  │  LLM         │ генерирует Response
└──────────────┘     └──────┬───────┘
                           ↓
┌──────────────┐     ┌──────────────┐
│ Сравнение    │ ←─  │ Response     │ vs эталон
│ loss         │     └──────────────┘
└──────┬───────┘
       ↓
┌──────────────┐
│ Обновляем A,B│ градиентный спуск
└──────────────┘
```

#### Фаза 3: Прорыв ← ✨ ВОТ ЗДЕСЬ МАГИЯ!
```
┌──────────────┐
│  4-bit + LoRA│ Mistral-7B в Colab T4
└──────┬───────┘
       ↓
✨ Модель влезает, обучается за часы, адаптер 50 MB
   Full FT = месяцы и сотни GPU; LoRA = 1 GPU, часы
```

---

## 📊 Таблица контрастов

| Что думают ❌ | Что на самом деле ✅ | Визуальная метафора |
|---------------|---------------------|----------------------|
| «Обучение = все веса» | LoRA — 0.05% весов, почти тот же эффект | `Шина vs весь автомобиль` |
| «Нужны сотни GPU» | 1 GPU (Colab T4) + 4-bit достаточно | `Телефон vs дата-центр` |
| «Адаптер = тяжёлый» | ~50 MB vs 14 GB базовой модели | `Закладка vs вся книга` |
| «Merge = потеря гибкости» | Merge = один файл для продакшена без PEFT | `Сборка vs разборный конструктор` |

---

## 💻 Мини-код с комментариями-стрелками

```python
# ← 1. Установка (Colab)
!pip install -q transformers datasets peft accelerate bitsandbytes trl
# ← 2. Проверка GPU
import torch
print(torch.cuda.get_device_name(0))  # T4, A100...
# ← 3. LoRA = малые матрицы в attention
from peft import LoraConfig, get_peft_model
config = LoraConfig(r=8, lora_alpha=32, ...)
#        ↑
#        └─── ✨ ВОТ ЗДЕСЬ МАГИЯ: r=8 — ранг, ~0.05% параметров
```

---

## ✅ Чек-лист самопроверки

```
✅ Проверь себя за 15 секунд:
▫️ Могу объяснить LoRA через метафору «донастройка переводчика»
▫️ Знаю разницу: Full FT vs LoRA (все веса vs 0.05%)
▫️ Понимаю 4-bit: модель влезает в Colab T4
▫️ Знаю, что адаптер ~50 MB, базовую модель можно не хранить
▫️ Могу загрузить LoRA и сделать merge для деплоя

→ Если 4+ галочки — уверенно владеешь основой.
```

---

## 🔍 Микро-проверка

**Вопрос:** Почему LoRA, а не обучать все веса?  
**Ответ:** Mistral-7B = 7 млрд параметров. Full FT = 80+ GB GPU, месяцы. LoRA = 0.05% весов, 16 GB, часы. Почти тот же эффект при 20× экономии.

**Вопрос:** Что такое merge и зачем?  
**Ответ:** Merge = «вшить» адаптер в base. Один файл модели для деплоя. Без PEFT-библиотеки. Но переключение адаптеров недоступно.

---

## ⚠️ Частые ошибки

| Ошибка | Решение |
|--------|---------|
| OOM (Out of Memory) в Colab | 4-bit квантизация, уменьши batch_size, gradient_accumulation |
| Адаптер «не помогает» — ответ как у базовой | Проверь, что загружаешь с `PeftModel`, а не просто base |

---

## ➡️ Что дальше

- **ДЗ 8** — Fine-tuning + Tools (инструктивный помощник).
- **adapters_colab** — переключение, merge, роутер.

**≈1 нед** — уроки 1–4 | **≈2–3 нед** — свой проект с merge

---

## 📁 Структура курса (7 уроков)

| Урок | Тема | Colab |
|------|------|-------|
| 0 | Стартер + GPU | `00_colab_starter.ipynb` |
| 1 | Введение в PEFT | `01_intro_peft.ipynb` |
| 2 | LoRA — математика | `02_lora_basics.ipynb` |
| 3 | Классификатор PEFT | `03_classifier_peft.ipynb` |
| 4 | LLM fine-tune | `04_llm_finetune.ipynb` |
| 5 | Продвинутые PEFT | `05_advanced_peft.ipynb` |
| 6 | Colab оптимизация | `06_colab_optimization.ipynb` |
| 7 | Деплой и инференс | `07_deploy_inference.ipynb` |

---

## 🚀 Быстрый старт

```python
!pip install -q transformers datasets peft accelerate bitsandbytes trl
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Память: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
```

---

**Порядок:** 00 → 01–02 → 03 → 04 → 05–07. Colab Pro ускоряет в 3–5 раз.
