# Модуль 1 — Фундамент: подробный разбор

Этот файл — подробная теоретическая и практическая карта модуля 1.  
Он объясняет **зачем** нужен каждый блокнот, **как** работает ключевая техника и **как применять** её в реальных задачах.

---

## 1) `01_llm_vs_chatmodel.ipynb`

### Теория
**Completion‑модели** — исторически первый интерфейс LLM: вы даёте строковый промпт, модель продолжает текст.  
**Chat‑модели** — эволюция completion‑интерфейса: вместо одной строки вы передаёте набор сообщений с ролями (`system`, `user`, `assistant`).  
Это даёт:
- более управляемый контекст (можно отдельно задать правила/роль модели);
- более стабильный формат ответа;
- лучший контроль безопасности и стиля.

**Исторически:**  
Completion‑подход доминировал до появления массовых чат‑интерфейсов.  
С развитием мультиролевых промптов (System/User/Assistant) индустрия перешла к chat‑моделям как к более управляемому стандарту.

### Практика в блокноте
Вы сравниваете **одну и ту же задачу** двумя способами:
- через `OpenAI` (completion) — если провайдер поддерживает completion‑модели;
- через `ChatOpenAI` (chat) — основной путь для большинства современных моделей.

**Ключевые параметры:**
- `model` — имя модели;
- `temperature` — степень «креативности»;
- `max_tokens` — ограничение длины ответа;
- `base_url` — если используется прокси (например, AITunnel).

### Когда это применять
Используйте chat‑модели в 95% случаев.  
Completion‑модель полезна как учебный контраст или для устаревших интеграций.

---

## 2) `02_prompt_template_fewshot.ipynb`

### Теория
**PromptTemplate** — шаблон, который позволяет убрать хардкод промптов и сделать их воспроизводимыми.  
Идея: отделить «форму» от «данных». Тогда можно менять входные значения, не трогая структуру.

**Few‑Shot** — обучение на нескольких примерах внутри промпта.  
Модель видит стиль/формат и повторяет его на новом входе.

**Исторически:**  
Few‑shot prompting стал стандартом до появления строгих структурированных инструментов (например, функции).  
Он остаётся полезным, когда нужно «настроить стиль» быстро и без обучения модели.

### Практика в блокноте
1) **PromptTemplate**  
Вы задаёте шаблон описания товара и подставляете разные значения.  
Это даёт стабильный формат и повторяемость.

2) **Few‑Shot**  
Вы задаёте пару эталонных примеров, чтобы модель воспроизводила стиль.  
Особенно полезно для:
- маркетинговых текстов;
- описаний товара;
- правил форматирования.

**Ключевые параметры:**
- `template` и `input_variables` — структура и переменные;
- `examples`, `example_prompt`, `suffix` — как формируются few‑shot примеры.

### Практические рекомендации
- 2–5 примеров достаточно; больше не всегда лучше.
- Примеры должны быть «типичными» и отражать нужный стиль.
- Чётко фиксируйте формат: модель любит повторять структуру.

---

## 3) `03_output_parser.ipynb`

### Теория
**OutputParser** нужен, чтобы превращать неструктурированный текст в предсказуемый объект.  
В продакшене это критично: бизнес‑логика зависит от типизированных данных.

**PydanticOutputParser** использует Pydantic‑схему и валидирует ответ.  
Если модель нарушит формат — вы получите ошибку, а не «мусорные» данные.

**Исторически:**  
Парсинг ответов стал стандартом, когда LLM начали интегрировать в реальные системы: CRM, бэкенды, аналитика.  
Без валидации модели часто ломают формат, и это приводит к багам.

### Практика в блокноте
1) Определяется схема `ProductInfo` с типами.  
2) Генерируются `format_instructions`.  
3) Модель обязана вернуть JSON строго по схеме.  
4) Ответ парсится; ошибки перехватываются.

**Ключевые параметры:**
- `BaseModel`/`Field` — схема и описание полей;
- `parser.get_format_instructions()` — инструкция модели;
- `OutputParserException` — ловим ошибки.

### Практические рекомендации
- Всегда требуйте «только JSON» без лишнего текста.
- Снижайте `temperature` для стабильного формата.
- Логируйте сырой ответ для отладки.

---

## 4) `04_local_models.ipynb`

### Теория
**Локальные модели** позволяют работать без внешних API:  
это полезно, когда важна приватность, офлайн‑режим или контроль затрат.

Минусы:
- нужно GPU/CPU ресурсы;
- качество ниже у компактных моделей;
- требуется время на загрузку и настройку.

**Исторически:**  
До эпохи массовых API многие команды запускали локальные модели, но это требовало сильной инфраструктуры.  
Сегодня локальные модели снова популярны из‑за вопросов приватности и стоимости.

### Практика в блокноте
Вы запускаете компактную русскоязычную модель через `transformers`:
- проверяется доступность GPU;
- загружается модель;
- делается генерация текста.

### Практические рекомендации
- Для Colab лучше брать небольшие модели (до 1–3B).
- Используйте `device_map="auto"` и GPU для скорости.
- Для продакшена часто нужны оптимизации (квантование, ускоренные рантаймы).

---

## Как использовать модуль 1

1. Начните с `01_llm_vs_chatmodel.ipynb` — получите базовое понимание интерфейсов.  
2. Перейдите к `02_prompt_template_fewshot.ipynb` — научитесь делать промпты управляемыми.  
3. Освойте `03_output_parser.ipynb` — это фундамент для продакшен‑сценариев.  
4. Завершите `04_local_models.ipynb`, чтобы понимать офлайн‑альтернативы.

---

## Итог модуля 1

К концу модуля вы:
- понимаете разницу completion vs chat;
- умеете создавать воспроизводимые промпты;
- получаете структурированные ответы;
- знаете, когда выгодны локальные модели.
