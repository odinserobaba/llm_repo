# Модуль 1 — Фундамент: карта понимания

Это как научиться писать аккуратно: сначала ручка, потом буквы, потом слова.

Здесь подробная и понятная карта модуля 1: что мы делали и зачем.

---

## 1) Зачем этот модуль

- понять разницу chat vs completion  
- научиться управлять промптом  
- получать структурированный ответ  
- попробовать локальные модели  

---

## 2) Блок A — интерфейсы модели

### `01_llm_vs_chatmodel.ipynb`
**Якорь:** как говорить с человеком vs писать ему письмо.  

**Что делаем:**
- пробуем completion  
- пробуем chat  
- сравниваем ответы  
- фиксируем параметры  

**Мини‑схема**
```
Completion: текст → продолжение
Chat: роли → ответ
```

**Микро‑упражнение:**  
Сравни ответы при `temperature=0.2` и `temperature=0.8`.

**Почему важно:** chat‑модели дают больше контроля и стабильности.

---

## 3) Блок B — управляемые промпты

### `02_prompt_template_fewshot.ipynb`
**Якорь:** как рецепт с пустыми строками для ингредиентов.  

**Что делаем:**
- делаем PromptTemplate  
- подставляем значения  
- задаём few‑shot примеры  
- проверяем стиль  

**Мини‑схема**
```
Шаблон + данные → готовый промпт
```

**Микро‑упражнение:**  
Добавь третий пример few‑shot и посмотри, как меняется стиль.

**Почему важно:** без шаблона промпт быстро превращается в хаос.

---

## 4) Блок C — структурированный ответ

### `03_output_parser.ipynb`
**Якорь:** как заполнять анкету по полям.  

**Что делаем:**
- задаём схему (Pydantic)  
- генерируем инструкцию формата  
- получаем JSON  
- ловим ошибки  

**Мини‑схема**
```
Схема → Инструкция → JSON
```

**Микро‑упражнение:**  
Сломай формат ответа и посмотри, где падает парсер.

**Почему важно:** бизнес‑логика не любит «свободный текст».

---

## 5) Блок D — локальные модели

### `04_local_models.ipynb`
**Якорь:** как готовить дома вместо ресторана.  

**Что делаем:**
- проверяем GPU  
- загружаем модель  
- генерируем текст  
- сравниваем скорость  

**Мини‑схема**
```
Модель → GPU → Ответ
```

**Микро‑упражнение:**  
Поменяй `max_new_tokens` и сравни длину ответа.

**Почему важно:** локальные модели нужны для офлайн‑режима и приватности.

---

## 6) Мини‑примеры кода (ещё подробнее)

### Пример 1: Chat‑вызов
```python
llm = ChatOpenAI(model="gpt-5-mini", temperature=0.2)
msg = "Сделай короткий заголовок"
answer = llm.invoke(msg)
# Схема: сообщение -> ответ
```
**Почему важно:** chat‑модель — основной путь в большинстве задач.

### Пример 2: Completion‑вызов
```python
llm = OpenAI(model="gpt-3.5-turbo-instruct")
answer = llm.invoke("Сделай заголовок:")
# Схема: строка -> продолжение
```
**Почему важно:** completion полезен для совместимости и контраста.

### Пример 3: PromptTemplate
```python
template = "Товар: {name}. Опиши в 1 фразе."
prompt = PromptTemplate(template=template, input_variables=["name"])
ready = prompt.format(name="Термокружка")
# Схема: шаблон + данные -> промпт
```
**Почему важно:** шаблон даёт стабильный формат.

### Пример 4: Few‑shot
```python
examples = [
    {"input": "яблоко", "output": "сочный фрукт"},
    {"input": "стол", "output": "мебель для работы"},
]
# Схема: примеры -> стиль
```
**Почему важно:** few‑shot быстро задаёт стиль ответа.

### Пример 5: OutputParser
```python
parser = PydanticOutputParser(pydantic_object=ProductInfo)
instructions = parser.get_format_instructions()
# Схема: схема -> инструкция
```
**Почему важно:** без парсера ответ нестабилен.

---

## 7) Микро‑проверка (с ответом)

Вопрос: что даёт PromptTemplate?  
Ответ: стабильную форму промпта при разных входных данных.

---

## 8) Итог модуля (4 результата)

- понимаешь chat vs completion  
- делаешь управляемые промпты  
- получаешь структурированные ответы  
- знаешь, когда нужны локальные модели  

---

Попробуй: открой `01_llm_vs_chatmodel.ipynb` и пройди его сверху вниз.
