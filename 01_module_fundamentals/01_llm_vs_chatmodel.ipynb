{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Модуль 1.1 — LLM vs ChatModel\n",
        "\n",
        "**Цель:** понять разницу между моделью completion (`OpenAI`) и chat-моделью (`ChatOpenAI`) на простом примере.\n",
        "Если completion‑модель у провайдера недоступна, используем локальную completion‑модель в Colab как альтернативу.\n",
        "\n",
        "**Что сделаем:**\n",
        "- установим нужные библиотеки прямо в блокноте\n",
        "- настроим API‑ключ\n",
        "- сравним ответы completion и chat\n",
        "- (опционально) проверим локальную completion‑модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install -U \\\n",
        "  langchain \\\n",
        "  langchain-openai \\\n",
        "  langchain-community \\\n",
        "  pydantic==2.12.3 \\\n",
        "  python-dotenv \\\n",
        "  tiktoken \\\n",
        "  requests==2.32.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Настройка ключа и base URL\n",
        "\n",
        "В Colab можно задать `OPENAI_API_KEY` через ввод или использовать `.env` файл, если вы его загрузили в сессию.\n",
        "\n",
        "Если вы ходите через AITunnel, задайте `OPENAI_BASE_URL` (например, `https://api.aitunnel.ru/v1/`) и используйте модели `gpt-5-mini` или `gpt-5-nano`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Введите OPENAI_API_KEY: \")\n",
        "\n",
        "# Для AITunnel (или другого OpenAI-совместимого прокси)\n",
        "if not os.environ.get(\"OPENAI_BASE_URL\"):\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = \"https://api.aitunnel.ru/v1/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Сравнение: completion vs chat\n",
        "\n",
        "**Идея:** одна и та же задача, два разных интерфейса.\n",
        "\n",
        "- `OpenAI` принимает строковый промпт и возвращает текст.\n",
        "- `ChatOpenAI` работает с сообщениями (`HumanMessage`, `SystemMessage`) и дает больше контроля над ролями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt = \"Сгенерируй 5 вариантов заголовка статьи про практическое применение LangChain.\"\n",
        "\n",
        "# Можно переключать на более дешевые модели при необходимости скорости/стоимости\n",
        "chat_model_name = \"gpt-5.2-chat\"  # альтернатива: \"gpt-5-mini\"\n",
        "\n",
        "# Completion-модель зависит от провайдера (часто не поддерживается в прокси)\n",
        "# Если у вас есть доступ к completion‑модели, укажите ее здесь\n",
        "completion_model_name = None  # например: \"gpt-3.5-turbo-instruct\"\n",
        "\n",
        "base_url = os.environ.get(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# Chat-модель\n",
        "chat = ChatOpenAI(\n",
        "    model=chat_model_name,\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    base_url=base_url,\n",
        ")\n",
        "\n",
        "chat_result = chat.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "print(\"ChatModel:\")\n",
        "print(chat_result.content)\n",
        "\n",
        "if completion_model_name:\n",
        "    llm = OpenAI(\n",
        "        model=completion_model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=256,\n",
        "        base_url=base_url,\n",
        "    )\n",
        "    llm_result = llm.invoke(prompt)\n",
        "    print(\"\\nLLM (completion):\")\n",
        "    print(llm_result)\n",
        "else:\n",
        "    print(\"\\nLLM (completion): пропущено — у провайдера нет completion‑модели\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Прайс‑гайд и переключение моделей\n",
        "\n",
        "**Рекомендация по умолчанию:** `gpt-5.2-chat` — лучший баланс качества для обучения и объяснений.\n",
        "\n",
        "**Когда переключаться на `gpt-5-mini`:**\n",
        "- много быстрых итераций (черновики, мозговые штурмы)\n",
        "- простой контент без сложных рассуждений\n",
        "- нужно снизить цену при сохранении приличного качества\n",
        "\n",
        "**Когда переключаться на `gpt-5-nano`:**\n",
        "- массовые прогоны, дешевые проверки\n",
        "- короткие ответы и шаблонные задачи\n",
        "- важна скорость и минимальная стоимость\n",
        "\n",
        "**Пример цен (по списку провайдера):**\n",
        "- `gpt-5.2-chat`: 33.6 ₽ / 1M входных токенов\n",
        "- `gpt-5-mini`: 4.8 ₽ / 1M входных токенов\n",
        "- `gpt-5-nano`: 0.96 ₽ / 1M входных токенов\n",
        "\n",
        "**Как переключить модель:**\n",
        "\n",
        "```python\n",
        "chat_model_name = \"gpt-5.2-chat\"  # или \"gpt-5-mini\" / \"gpt-5-nano\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Опционально: локальная completion‑модель в Colab\n",
        "\n",
        "Если ваш провайдер не поддерживает completion‑модели, можно использовать локальную causal‑LM как аналог completion.\n",
        "Ниже — минимальный пример через `transformers`.\n",
        "\n",
        "> Примечание: модель будет скачана в среду Colab, это может занять несколько минут."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install -U transformers accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "local_model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
        "\n",
        "local_tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
        "local_model = AutoModelForCausalLM.from_pretrained(\n",
        "    local_model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "local_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=local_model,\n",
        "    tokenizer=local_tokenizer,\n",
        ")\n",
        "\n",
        "local_prompt = \"Сгенерируй 3 варианта заголовка статьи про практическое применение LangChain.\"\n",
        "local_result = local_generator(local_prompt, max_new_tokens=80, do_sample=True, temperature=0.7)\n",
        "\n",
        "print(local_result[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
