{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ú–æ–¥—É–ª—å 1.1 ‚Äî LLM vs ChatModel\n",
        "\n",
        "**–¶–µ–ª—å:** –ø–æ–Ω—è—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é completion (`OpenAI`) –∏ chat-–º–æ–¥–µ–ª—å—é (`ChatOpenAI`) –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ.\n",
        "–ï—Å–ª–∏ completion‚Äë–º–æ–¥–µ–ª—å —É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é completion‚Äë–º–æ–¥–µ–ª—å –≤ Colab –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É.\n",
        "\n",
        "**–ß—Ç–æ —Å–¥–µ–ª–∞–µ–º:**\n",
        "- —É—Å—Ç–∞–Ω–æ–≤–∏–º –Ω—É–∂–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø—Ä—è–º–æ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ\n",
        "- –Ω–∞—Å—Ç—Ä–æ–∏–º API‚Äë–∫–ª—é—á\n",
        "- —Å—Ä–∞–≤–Ω–∏–º –æ—Ç–≤–µ—Ç—ã completion –∏ chat\n",
        "- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø—Ä–æ–≤–µ—Ä–∏–º –ª–æ–∫–∞–ª—å–Ω—É—é completion‚Äë–º–æ–¥–µ–ª—å"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üéØ –Ø–∫–æ—Ä—å\n",
        "**Completion** = ¬´–ø—Ä–æ–¥–æ–ª–∂–∏ —Å—Ç—Ä–æ–∫—É¬ª (–∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ —Ç–µ–ª–µ—Ñ–æ–Ω–µ).  \n",
        "**Chat** = –¥–∏–∞–ª–æ–≥ —Å —Ä–æ–ª—è–º–∏ system/user/assistant (–ø–µ—Ä–µ–ø–∏—Å–∫–∞ –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–µ).\n",
        "\n",
        "–¢–µ—Ä–º–∏–Ω—ã: **LLM** ‚Äî –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å; **invoke** ‚Äî –≤—ã–∑–æ–≤; **temperature** ‚Äî ¬´–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å¬ª 0‚Äì1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip -q install -U \\\n",
        "  langchain \\\n",
        "  langchain-openai \\\n",
        "  langchain-community \\\n",
        "  pydantic==2.12.3 \\\n",
        "  python-dotenv \\\n",
        "  tiktoken \\\n",
        "  requests==2.32.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª—é—á–∞ –∏ base URL\n",
        "\n",
        "–í Colab –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å `OPENAI_API_KEY` —á–µ—Ä–µ–∑ –≤–≤–æ–¥ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `.env` —Ñ–∞–π–ª, –µ—Å–ª–∏ –≤—ã –µ–≥–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –≤ —Å–µ—Å—Å–∏—é.\n",
        "\n",
        "–ï—Å–ª–∏ –≤—ã —Ö–æ–¥–∏—Ç–µ —á–µ—Ä–µ–∑ AITunnel, –∑–∞–¥–∞–π—Ç–µ `OPENAI_BASE_URL` (–Ω–∞–ø—Ä–∏–º–µ—Ä, `https://api.aitunnel.ru/v1/`) –∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ `gpt-5-mini` –∏–ª–∏ `gpt-5-nano`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"–í–≤–µ–¥–∏—Ç–µ OPENAI_API_KEY: \")\n",
        "\n",
        "# –î–ª—è AITunnel (–∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ –ø—Ä–æ–∫—Å–∏)\n",
        "if not os.environ.get(\"OPENAI_BASE_URL\"):\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = \"https://api.aitunnel.ru/v1/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: completion vs chat\n",
        "\n",
        "**–ò–¥–µ—è:** –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –∑–∞–¥–∞—á–∞, –¥–≤–∞ —Ä–∞–∑–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.\n",
        "\n",
        "- `OpenAI` –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç.\n",
        "- `ChatOpenAI` —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ (`HumanMessage`, `SystemMessage`) –∏ –¥–∞–µ—Ç –±–æ–ª—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ —Ä–æ–ª—è–º–∏."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt = \"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 5 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LangChain.\"\n",
        "\n",
        "# –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å –Ω–∞ –±–æ–ª–µ–µ –¥–µ—à–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏/—Å—Ç–æ–∏–º–æ—Å—Ç–∏\n",
        "chat_model_name = \"gpt-5.2-chat\"  # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: \"gpt-5-mini\"\n",
        "\n",
        "# Completion-–º–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (—á–∞—Å—Ç–æ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ –ø—Ä–æ–∫—Å–∏)\n",
        "# –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ completion‚Äë–º–æ–¥–µ–ª–∏, —É–∫–∞–∂–∏—Ç–µ –µ–µ –∑–¥–µ—Å—å\n",
        "completion_model_name = None  # –Ω–∞–ø—Ä–∏–º–µ—Ä: \"gpt-3.5-turbo-instruct\"\n",
        "\n",
        "base_url = os.environ.get(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# Chat-–º–æ–¥–µ–ª—å\n",
        "chat = ChatOpenAI(\n",
        "    model=chat_model_name,\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    base_url=base_url,\n",
        ")\n",
        "\n",
        "chat_result = chat.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "print(\"ChatModel:\")\n",
        "print(chat_result.content)\n",
        "\n",
        "if completion_model_name:\n",
        "    llm = OpenAI(\n",
        "        model=completion_model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=256,\n",
        "        base_url=base_url,\n",
        "    )\n",
        "    llm_result = llm.invoke(prompt)\n",
        "    print(\"\\nLLM (completion):\")\n",
        "    print(llm_result)\n",
        "else:\n",
        "    print(\"\\nLLM (completion): –ø—Ä–æ–ø—É—â–µ–Ω–æ ‚Äî —É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–µ—Ç completion‚Äë–º–æ–¥–µ–ª–∏\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üîç –ú–∏–∫—Ä–æ-–ø—Ä–æ–≤–µ—Ä–∫–∞\n",
        "**–í–æ–ø—Ä–æ—Å:** –ß–µ–º Chat –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç Completion?  \n",
        "–ü–æ–¥—É–º–∞–π –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º, –∑–∞—Ç–µ–º —Å–≤–µ—Ä—å—Å—è —Å README –º–æ–¥—É–ª—è 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ü—Ä–∞–π—Å‚Äë–≥–∞–π–¥ –∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é:** `gpt-5.2-chat` ‚Äî –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.\n",
        "\n",
        "**–ö–æ–≥–¥–∞ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –Ω–∞ `gpt-5-mini`:**\n",
        "- –º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π (—á–µ—Ä–Ω–æ–≤–∏–∫–∏, –º–æ–∑–≥–æ–≤—ã–µ —à—Ç—É—Ä–º—ã)\n",
        "- –ø—Ä–æ—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π\n",
        "- –Ω—É–∂–Ω–æ —Å–Ω–∏–∑–∏—Ç—å —Ü–µ–Ω—É –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–∏–ª–∏—á–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "\n",
        "**–ö–æ–≥–¥–∞ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –Ω–∞ `gpt-5-nano`:**\n",
        "- –º–∞—Å—Å–æ–≤—ã–µ –ø—Ä–æ–≥–æ–Ω—ã, –¥–µ—à–µ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
        "- –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –∏ —à–∞–±–ª–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏\n",
        "- –≤–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä —Ü–µ–Ω (–ø–æ —Å–ø–∏—Å–∫—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞):**\n",
        "- `gpt-5.2-chat`: 33.6 ‚ÇΩ / 1M –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
        "- `gpt-5-mini`: 4.8 ‚ÇΩ / 1M –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
        "- `gpt-5-nano`: 0.96 ‚ÇΩ / 1M –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
        "\n",
        "**–ö–∞–∫ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å:**\n",
        "\n",
        "```python\n",
        "chat_model_name = \"gpt-5.2-chat\"  # –∏–ª–∏ \"gpt-5-mini\" / \"gpt-5-nano\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ª–æ–∫–∞–ª—å–Ω–∞—è completion‚Äë–º–æ–¥–µ–ª—å –≤ Colab\n",
        "\n",
        "–ï—Å–ª–∏ –≤–∞—à –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç completion‚Äë–º–æ–¥–µ–ª–∏, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é causal‚ÄëLM –∫–∞–∫ –∞–Ω–∞–ª–æ–≥ completion.\n",
        "–ù–∏–∂–µ ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä —á–µ—Ä–µ–∑ `transformers`.\n",
        "\n",
        "> –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω–∞ –≤ —Å—Ä–µ–¥—É Colab, —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip -q install -U transformers accelerate sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "local_model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
        "\n",
        "local_tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
        "local_model = AutoModelForCausalLM.from_pretrained(\n",
        "    local_model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "local_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=local_model,\n",
        "    tokenizer=local_tokenizer,\n",
        ")\n",
        "\n",
        "local_prompt = \"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LangChain.\"\n",
        "local_result = local_generator(local_prompt, max_new_tokens=80, do_sample=True, temperature=0.7)\n",
        "\n",
        "print(local_result[0][\"generated_text\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}