{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ДЗ 9: Information Extraction из новостных диалогов\n",
        "\n",
        "**Трек B** — извлечение сущностей (PERSON, ORG, LOC, EVENT, DATE, IMPACT, SOURCE) из диалогов с помощью LLM.\n",
        "\n",
        "**Этапы:**\n",
        "1. Локальное развертывание моделей (quantized vs full)\n",
        "2. Подготовка данных (WildChat-1M)\n",
        "3. Оптимизация для IE (batch processing)\n",
        "4. Анализ производительности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Установка зависимостей\n",
        "\n",
        "> В Colab: **Runtime → Change runtime type → GPU (T4)**. После pip может понадобиться Restart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Без pipeline → меньше зависимостей, нет ошибки torchvision::nms\n",
        "!pip install -q -U transformers accelerate bitsandbytes datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Загрузка данных (WildChat-1M)\n",
        "\n",
        "Датасет: 1M диалогов человек–ChatGPT. Подвыборка 500–1K для CPU, 1K–2K для GPU. Demo: 100–200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "N_SAMPLES = 200  # Demo: 200. Для полного: 1000 или 2000\n",
        "\n",
        "# split='train[:N]' загружает только N примеров (экономит время и диск)\n",
        "ds = load_dataset(\"allenai/WildChat-1M\", split=f\"train[:{N_SAMPLES}]\")\n",
        "\n",
        "def get_conversation_text(sample):\n",
        "    conv = sample.get(\"conversation\", [])\n",
        "    parts = []\n",
        "    for turn in conv:\n",
        "        c = turn.get(\"content\", \"\")\n",
        "        if c:\n",
        "            parts.append(c.strip())\n",
        "    return \" \\n \".join(parts) if parts else \"\"\n",
        "\n",
        "texts = [get_conversation_text(ds[i]) for i in range(len(ds))]\n",
        "texts = [t for t in texts if len(t) > 50]  # фильтр коротких\n",
        "print(f\"Загружено {len(texts)} диалогов\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Промпт для IE\n",
        "\n",
        "Извлечение сущностей в JSON: PERSON, ORG, LOC, EVENT, DATE, IMPACT, SOURCE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IE_PROMPT = \"\"\"Extract entities from the text. Return JSON with keys: PERSON, ORG, LOC, EVENT, DATE, IMPACT, SOURCE. Each key is a list of strings. If nothing found, use empty list []. Output ONLY valid JSON, no other text.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "def make_ie_prompt(text, max_chars=1500, model_type=\"mistral\"):\n",
        "    t = text[:max_chars] if len(text) > max_chars else text\n",
        "    body = IE_PROMPT.format(text=t)\n",
        "    if model_type == \"mistral\":\n",
        "        return f\"<s>[INST] {body} [/INST]\"\n",
        "    if model_type == \"tinyllama\":\n",
        "        return f\"<|system|>\\nYou are a helpful assistant.<|user|>\\n{body}<|assistant|>\\n\"\n",
        "    return body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Модели: TinyLlama (full/4-bit) и Mistral (4-bit)\n",
        "\n",
        "Сравнение: quantized vs full precision по скорости и памяти."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# Используем только Auto* — без pipeline, чтобы избежать torchvision::nms в Colab\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "def load_model(name, use_4bit=False):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    if use_4bit:\n",
        "        bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "        model = AutoModelForCausalLM.from_pretrained(name, quantization_config=bnb, device_map=\"auto\")\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Конфиг: какая модель (для Colab T4 — tiny 4-bit или mistral 4-bit)\n",
        "USE_MISTRAL = True  # True = Mistral 4-bit (медленнее, качественнее), False = TinyLlama\n",
        "\n",
        "if USE_MISTRAL:\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    model, tokenizer = load_model(model_id, use_4bit=True)\n",
        "else:\n",
        "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    model, tokenizer = load_model(model_id, use_4bit=False)\n",
        "\n",
        "# Вместо pipeline используем model.generate() напрямую\n",
        "def generate(pipe_model, pipe_tokenizer, prompt, max_new_tokens=256):\n",
        "    inputs = pipe_tokenizer(prompt, return_tensors=\"pt\").to(pipe_model.device)\n",
        "    out = pipe_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=pipe_tokenizer.eos_token_id)\n",
        "    return pipe_tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "pipe = (model, tokenizer)  # (model, tokenizer) для совместимости с extract_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. IE: единичный и batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import re\n",
        "\n",
        "def extract_entities(pipe, text, max_new_tokens=200, model_type=\"mistral\"):\n",
        "    model, tokenizer = pipe\n",
        "    prompt = make_ie_prompt(text, model_type=model_type)\n",
        "    raw = generate(model, tokenizer, prompt, max_new_tokens=max_new_tokens).strip()\n",
        "    # Извлекаем первый полный JSON-объект (модель может добавить пояснения после)\n",
        "    start = raw.find(\"{\")\n",
        "    if start >= 0:\n",
        "        depth, end = 0, None\n",
        "        for i, c in enumerate(raw[start:], start):\n",
        "            if c == \"{\": depth += 1\n",
        "            elif c == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    end = i + 1\n",
        "                    break\n",
        "        if end is not None:\n",
        "            try:\n",
        "                return json.loads(raw[start:end])\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "    return {\"raw\": raw}\n",
        "\n",
        "def run_ie_batch(pipe, texts, batch_size=1, model_type=\"mistral\"):\n",
        "    results = []\n",
        "    t0 = time.time()\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        for t in batch:\n",
        "            r = extract_entities(pipe, t, model_type=model_type)\n",
        "            results.append(r)\n",
        "    elapsed = time.time() - t0\n",
        "    return results, elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Запуск на подвыборке (10 для demo)\n",
        "N_RUN = min(10, len(texts))\n",
        "sample_texts = texts[:N_RUN]\n",
        "\n",
        "model_type = \"mistral\" if USE_MISTRAL else \"tinyllama\"\n",
        "results, elapsed = run_ie_batch(pipe, sample_texts, model_type=model_type)\n",
        "print(f\"Обработано {N_RUN} диалогов за {elapsed:.1f} сек\")\n",
        "print(f\"Throughput: {N_RUN/elapsed:.2f} диалогов/сек\")\n",
        "print()\n",
        "print(\"Пример извлечения:\")\n",
        "print(json.dumps(results[0], ensure_ascii=False, indent=2)[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Анализ производительности\n",
        "\n",
        "- Скорость: диалогов/сек\n",
        "- Ресурсы: VRAM (torch.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    vram_gb = torch.cuda.max_memory_allocated() / 1e9\n",
        "    print(f\"VRAM пик: {vram_gb:.2f} GB\")\n",
        "print(f\"Время на {N_RUN} диалогов: {elapsed:.1f} сек\")\n",
        "print(f\"Среднее: {elapsed/N_RUN:.2f} сек/диалог\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Анализ результатов IE\n",
        "\n",
        "Сводная статистика: доля валидного JSON, распределение сущностей по типам, примеры «вход → выход»."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Статистика: валидный JSON vs сырой вывод (когда парсер не смог извлечь JSON)\n",
        "valid_count = sum(1 for r in results if \"raw\" not in r)\n",
        "has_raw = [i for i, r in enumerate(results) if \"raw\" in r]\n",
        "print(f\"Валидный JSON: {valid_count}/{len(results)} ({100*valid_count/len(results):.0f}%)\")\n",
        "print(f\"Сырой вывод (модель дала не-JSON или битый JSON): {len(has_raw)}\")\n",
        "if has_raw:\n",
        "    print(\"Индексы:\", has_raw[:5], \"...\" if len(has_raw) > 5 else \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Распределение сущностей по типам (среди валидных)\n",
        "entity_keys = [\"PERSON\", \"ORG\", \"LOC\", \"EVENT\", \"DATE\", \"IMPACT\", \"SOURCE\"]\n",
        "counts = {k: 0 for k in entity_keys}\n",
        "total_entities = 0\n",
        "for r in results:\n",
        "    if \"raw\" in r:\n",
        "        continue\n",
        "    for k in entity_keys:\n",
        "        vals = r.get(k, [])\n",
        "        if isinstance(vals, list):\n",
        "            n = len(vals)\n",
        "        else:\n",
        "            n = 1 if vals else 0\n",
        "        counts[k] += n\n",
        "        total_entities += n\n",
        "\n",
        "print(\"Сущностей по типам:\")\n",
        "for k, v in sorted(counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"Всего извлечено: {total_entities}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Примеры: входной текст → извлечённые сущности (2–3 примера)\n",
        "indices = [0]\n",
        "if len(results) > 1:\n",
        "    indices.append(1)\n",
        "if len(results) > 3:\n",
        "    indices.append(len(results) // 2)\n",
        "for idx in indices:\n",
        "    if idx >= len(sample_texts):\n",
        "        break\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Пример {idx+1}\")\n",
        "    print(\"-\" * 40)\n",
        "    txt = sample_texts[idx]\n",
        "    print(\"Вход (сокращённо):\", txt[:300] + \"...\" if len(txt) > 300 else txt)\n",
        "    print()\n",
        "    r = results[idx]\n",
        "    if \"raw\" in r:\n",
        "        print(\"Вывод (raw):\", r[\"raw\"][:400] + \"...\" if len(r[\"raw\"]) > 400 else r[\"raw\"])\n",
        "    else:\n",
        "        print(\"Извлечено:\")\n",
        "        for k in entity_keys:\n",
        "            vals = r.get(k, [])\n",
        "            if vals:\n",
        "                print(f\"  {k}: {vals}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Итоговая таблица производительности\n",
        "print(\"Итоги производительности\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Модель: {model_id}\")\n",
        "print(f\"Диалогов обработано: {N_RUN}\")\n",
        "print(f\"Время: {elapsed:.1f} сек\")\n",
        "print(f\"Скорость: {N_RUN/elapsed:.3f} диалогов/сек (~{elapsed/N_RUN:.1f} сек/диалог)\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"VRAM пик: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Что смотреть и возможные улучшения\n",
        "\n",
        "| Что проверять | Зачем |\n",
        "|---------------|-------|\n",
        "| Доля валидного JSON | Низкая → добавить в промпт «только JSON, без текста после» |\n",
        "| PERSON/ORG/LOC чаще пустые | Диалоги могут не содержать явных сущностей; попробовать другой датасет или фильтр |\n",
        "| Модель пишет пояснения после JSON | Явно указать: «Output ONLY valid JSON, no extra text» |\n",
        "| Очень медленно | TinyLlama быстрее; batch-запросы (несколько текстов в одном промпте) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Если много raw — попробуй добавить в IE_PROMPT: \"Output ONLY valid JSON, no other text.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Опционально: вторая модель для сравнения\n",
        "\n",
        "Перед запуском — освободить память (`del model`, `torch.cuda.empty_cache()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сравнение TinyLlama vs Mistral (запускать по очереди, не одновременно)\n",
        "# 1) TinyLlama ~2GB VRAM, быстрее\n",
        "# 2) Mistral 4-bit ~6GB VRAM, качественнее\n",
        "# Замерь время и VRAM для каждой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Опционально: системный промпт «когнитивный дизайнер»\n",
        "\n",
        "Для объяснений в стиле когнитивного дизайнера — см. `prompt_cognitive_designer.md` или `../promt.md`. Добавь в начало промпта перед запросом пользователя."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
