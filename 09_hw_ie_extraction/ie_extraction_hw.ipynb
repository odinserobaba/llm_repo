{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –î–ó 9: Information Extraction –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤\n",
        "\n",
        "**–¢—Ä–µ–∫ B** ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (PERSON, ORG, LOC, EVENT, DATE, IMPACT, SOURCE) –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM.\n",
        "\n",
        "**–≠—Ç–∞–ø—ã:**\n",
        "1. –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (quantized vs full)\n",
        "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (WildChat-1M)\n",
        "3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è IE (batch processing)\n",
        "4. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üéØ –Ø–∫–æ—Ä—å\n",
        "–ò–∑ —á–∞—Ç–∞ –æ –Ω–æ–≤–æ—Å—Ç—è—Ö –≤—ã—Ç–∞—â–∏—Ç—å: –∫—Ç–æ, –≥–¥–µ, –∫–æ–≥–¥–∞, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ. LLM ‚Üí JSON ‚Üí —Ç–∞–±–ª–∏—Ü–∞.\n",
        "\n",
        "**–¢–µ—Ä–º–∏–Ω—ã:** WildChat-1M | PERSON, ORG, LOC, EVENT | –ü–∞—Ä—Å–µ—Ä –ø–æ —Å–∫–æ–±–∫–∞–º | ¬´Output ONLY valid JSON¬ª"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "\n",
        "> –í Colab: **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**. –ü–æ—Å–ª–µ pip –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è Restart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ë–µ–∑ pipeline ‚Üí –º–µ–Ω—å—à–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, –Ω–µ—Ç –æ—à–∏–±–∫–∏ torchvision::nms\n",
        "!pip install -q -U transformers accelerate bitsandbytes datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (WildChat-1M)\n",
        "\n",
        "–î–∞—Ç–∞—Å–µ—Ç: 1M –¥–∏–∞–ª–æ–≥–æ–≤ —á–µ–ª–æ–≤–µ–∫‚ÄìChatGPT. –ë–µ—Ä—ë–º **—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ** –¥–∏–∞–ª–æ–≥–∏ (language=ru)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "N_SAMPLES = 200  # –ù—É–∂–Ω–æ —Ä—É—Å—Å–∫–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤. –ì—Ä—É–∑–∏–º –±–æ–ª—å—à–µ ‚Äî ~28% –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ —Ä—É—Å—Å–∫–∏–µ.\n",
        "LOAD_SIZE = 1000  # –ó–∞–≥—Ä—É–∑–∏—Ç—å, —á—Ç–æ–±—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —è–∑—ã–∫—É –æ—Å—Ç–∞–ª–æ—Å—å ~N_SAMPLES\n",
        "\n",
        "ds = load_dataset(\"allenai/WildChat-1M\", split=f\"train[:{LOAD_SIZE}]\")\n",
        "\n",
        "def get_conversation_text(sample):\n",
        "    conv = sample.get(\"conversation\", [])\n",
        "    parts = []\n",
        "    for turn in conv:\n",
        "        c = turn.get(\"content\", \"\")\n",
        "        if c:\n",
        "            parts.append(c.strip())\n",
        "    return \" \\n \".join(parts) if parts else \"\"\n",
        "\n",
        "# –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –¥–∏–∞–ª–æ–≥–∏ (—è–∑—ã–∫: ru)\n",
        "ru_indices = [i for i in range(len(ds)) if ds[i].get(\"language\") == \"ru\"]\n",
        "texts = []\n",
        "for i in ru_indices[:N_SAMPLES]:\n",
        "    t = get_conversation_text(ds[i])\n",
        "    if len(t) > 50:\n",
        "        texts.append(t)\n",
        "if not texts:\n",
        "    texts = [get_conversation_text(ds[i]) for i in range(min(100, len(ds)))]\n",
        "    texts = [t for t in texts if len(t) > 50]\n",
        "    print(f\"–†—É—Å—Å–∫–∏—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–∑—è—Ç—ã –ø–µ—Ä–≤—ã–µ {len(texts)} –¥–∏–∞–ª–æ–≥–æ–≤\")\n",
        "else:\n",
        "    print(f\"–†—É—Å—Å–∫–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤: {len(ru_indices)} –≤ –≤—ã–±–æ—Ä–∫–µ, –≤–∑—è—Ç–æ {len(texts)}\")\n",
        "print(f\"–ì–æ—Ç–æ–≤–æ: {len(texts)} –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è IE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ü—Ä–æ–º–ø—Ç –¥–ª—è IE\n",
        "\n",
        "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ JSON: PERSON, ORG, LOC, EVENT, DATE, IMPACT, SOURCE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IE_PROMPT = \"\"\"–ò–∑–≤–ª–µ–∫–∏ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞. –í–µ—Ä–Ω–∏ JSON —Å –∫–ª—é—á–∞–º–∏: PERSON, ORG, LOC, EVENT, DATE, IMPACT, SOURCE. –ö–∞–∂–¥—ã–π –∫–ª—é—á ‚Äî —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫. –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ []. –í—ã–≤–æ–¥–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.\n",
        "\n",
        "–¢–µ–∫—Å—Ç:\n",
        "{text}\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "def make_ie_prompt(text, max_chars=1500, model_type=\"mistral\"):\n",
        "    t = text[:max_chars] if len(text) > max_chars else text\n",
        "    body = IE_PROMPT.format(text=t)\n",
        "    if model_type == \"mistral\":\n",
        "        return f\"<s>[INST] {body} [/INST]\"\n",
        "    if model_type == \"tinyllama\":\n",
        "        return f\"<|system|>\\nYou are a helpful assistant.<|user|>\\n{body}<|assistant|>\\n\"\n",
        "    return body"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –ú–æ–¥–µ–ª–∏: TinyLlama (full/4-bit) –∏ Mistral (4-bit)\n",
        "\n",
        "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ: quantized vs full precision –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –ø–∞–º—è—Ç–∏."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Auto* ‚Äî –±–µ–∑ pipeline, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å torchvision::nms –≤ Colab\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "def load_model(name, use_4bit=False):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    if use_4bit:\n",
        "        bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "        model = AutoModelForCausalLM.from_pretrained(name, quantization_config=bnb, device_map=\"auto\")\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "    return model, tokenizer\n",
        "\n",
        "# –ö–æ–Ω—Ñ–∏–≥: –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å (–¥–ª—è Colab T4 ‚Äî tiny 4-bit –∏–ª–∏ mistral 4-bit)\n",
        "USE_MISTRAL = True  # True = Mistral 4-bit (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ), False = TinyLlama\n",
        "\n",
        "if USE_MISTRAL:\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    model, tokenizer = load_model(model_id, use_4bit=True)\n",
        "else:\n",
        "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    model, tokenizer = load_model(model_id, use_4bit=False)\n",
        "\n",
        "# –í–º–µ—Å—Ç–æ pipeline –∏—Å–ø–æ–ª—å–∑—É–µ–º model.generate() –Ω–∞–ø—Ä—è–º—É—é\n",
        "def generate(pipe_model, pipe_tokenizer, prompt, max_new_tokens=256):\n",
        "    inputs = pipe_tokenizer(prompt, return_tensors=\"pt\").to(pipe_model.device)\n",
        "    out = pipe_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=pipe_tokenizer.eos_token_id)\n",
        "    return pipe_tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "pipe = (model, tokenizer)  # (model, tokenizer) –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å extract_entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. IE: –µ–¥–∏–Ω–∏—á–Ω—ã–π –∏ batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "import json\n",
        "import re\n",
        "\n",
        "def extract_entities(pipe, text, max_new_tokens=200, model_type=\"mistral\"):\n",
        "    model, tokenizer = pipe\n",
        "    prompt = make_ie_prompt(text, model_type=model_type)\n",
        "    raw = generate(model, tokenizer, prompt, max_new_tokens=max_new_tokens).strip()\n",
        "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π –ø–æ–ª–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç (–º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ)\n",
        "    start = raw.find(\"{\")\n",
        "    if start >= 0:\n",
        "        depth, end = 0, None\n",
        "        for i, c in enumerate(raw[start:], start):\n",
        "            if c == \"{\": depth += 1\n",
        "            elif c == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    end = i + 1\n",
        "                    break\n",
        "        if end is not None:\n",
        "            try:\n",
        "                return json.loads(raw[start:end])\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "    return {\"raw\": raw}\n",
        "\n",
        "def run_ie_batch(pipe, texts, batch_size=1, model_type=\"mistral\"):\n",
        "    results = []\n",
        "    t0 = time.time()\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        for t in batch:\n",
        "            r = extract_entities(pipe, t, model_type=model_type)\n",
        "            results.append(r)\n",
        "    elapsed = time.time() - t0\n",
        "    return results, elapsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ó–∞–ø—É—Å–∫ –Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ (10 –¥–ª—è demo)\n",
        "N_RUN = min(10, len(texts))\n",
        "sample_texts = texts[:N_RUN]\n",
        "\n",
        "model_type = \"mistral\" if USE_MISTRAL else \"tinyllama\"\n",
        "results, elapsed = run_ie_batch(pipe, sample_texts, model_type=model_type)\n",
        "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {N_RUN} –¥–∏–∞–ª–æ–≥–æ–≤ –∑–∞ {elapsed:.1f} —Å–µ–∫\")\n",
        "print(f\"Throughput: {N_RUN/elapsed:.2f} –¥–∏–∞–ª–æ–≥–æ–≤/—Å–µ–∫\")\n",
        "print()\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:\")\n",
        "print(json.dumps(results[0], ensure_ascii=False, indent=2)[:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "- –°–∫–æ—Ä–æ—Å—Ç—å: –¥–∏–∞–ª–æ–≥–æ–≤/—Å–µ–∫\n",
        "- –†–µ—Å—É—Ä—Å—ã: VRAM (torch.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if torch.cuda.is_available():\n",
        "    vram_gb = torch.cuda.max_memory_allocated() / 1e9\n",
        "    print(f\"VRAM –ø–∏–∫: {vram_gb:.2f} GB\")\n",
        "print(f\"–í—Ä–µ–º—è –Ω–∞ {N_RUN} –¥–∏–∞–ª–æ–≥–æ–≤: {elapsed:.1f} —Å–µ–∫\")\n",
        "print(f\"–°—Ä–µ–¥–Ω–µ–µ: {elapsed/N_RUN:.2f} —Å–µ–∫/–¥–∏–∞–ª–æ–≥\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ IE\n",
        "\n",
        "–°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –¥–æ–ª—è –≤–∞–ª–∏–¥–Ω–æ–≥–æ JSON, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º, –ø—Ä–∏–º–µ—Ä—ã ¬´–≤—Ö–æ–¥ ‚Üí –≤—ã—Ö–æ–¥¬ª."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –≤–∞–ª–∏–¥–Ω—ã–π JSON vs —Å—ã—Ä–æ–π –≤—ã–≤–æ–¥ (–∫–æ–≥–¥–∞ –ø–∞—Ä—Å–µ—Ä –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å JSON)\n",
        "valid_count = sum(1 for r in results if \"raw\" not in r)\n",
        "has_raw = [i for i, r in enumerate(results) if \"raw\" in r]\n",
        "print(f\"–í–∞–ª–∏–¥–Ω—ã–π JSON: {valid_count}/{len(results)} ({100*valid_count/len(results):.0f}%)\")\n",
        "print(f\"–°—ã—Ä–æ–π –≤—ã–≤–æ–¥ (–º–æ–¥–µ–ª—å –¥–∞–ª–∞ –Ω–µ-JSON –∏–ª–∏ –±–∏—Ç—ã–π JSON): {len(has_raw)}\")\n",
        "if has_raw:\n",
        "    print(\"–ò–Ω–¥–µ–∫—Å—ã:\", has_raw[:5], \"...\" if len(has_raw) > 5 else \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º (—Å—Ä–µ–¥–∏ –≤–∞–ª–∏–¥–Ω—ã—Ö)\n",
        "entity_keys = [\"PERSON\", \"ORG\", \"LOC\", \"EVENT\", \"DATE\", \"IMPACT\", \"SOURCE\"]\n",
        "counts = {k: 0 for k in entity_keys}\n",
        "total_entities = 0\n",
        "for r in results:\n",
        "    if \"raw\" in r:\n",
        "        continue\n",
        "    for k in entity_keys:\n",
        "        vals = r.get(k, [])\n",
        "        if isinstance(vals, list):\n",
        "            n = len(vals)\n",
        "        else:\n",
        "            n = 1 if vals else 0\n",
        "        counts[k] += n\n",
        "        total_entities += n\n",
        "\n",
        "print(\"–°—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º:\")\n",
        "for k, v in sorted(counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"–í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {total_entities}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "sorted_items = sorted(counts.items(), key=lambda x: -x[1])\n",
        "labels = [k for k, _ in sorted_items]\n",
        "values = [v for _, v in sorted_items]\n",
        "colors = plt.cm.Set3(range(len(labels)))\n",
        "bars = ax.barh(labels, values, color=colors)\n",
        "ax.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π')\n",
        "ax.set_title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º\")\n",
        "for b, v in zip(bars, values):\n",
        "    ax.text(v + 0.3, b.get_y() + b.get_height()/2, str(v), va=\"center\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\n",
        "import pandas as pd\n",
        "summary = pd.DataFrame({\n",
        "    \"–ú–µ—Ç—Ä–∏–∫–∞\": [\"–ú–æ–¥–µ–ª—å\", \"–î–∏–∞–ª–æ–≥–æ–≤\", \"–í–∞–ª–∏–¥–Ω—ã–π JSON\", \"–í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π\", \"–í—Ä–µ–º—è (—Å–µ–∫)\", \"VRAM (GB)\"],\n",
        "    \"–ó–Ω–∞—á–µ–Ω–∏–µ\": [\n",
        "        model_id,\n",
        "        N_RUN,\n",
        "        f\"{valid_count}/{len(results)} ({100*valid_count/len(results):.0f}%)\",\n",
        "        total_entities,\n",
        "        f\"{elapsed:.1f}\",\n",
        "        f\"{torch.cuda.max_memory_allocated()/1e9:.2f}\" if torch.cuda.is_available() else \"‚Äî\"\n",
        "    ]\n",
        "})\n",
        "try:\n",
        "    display(summary)\n",
        "except:\n",
        "    print(summary.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ü—Ä–∏–º–µ—Ä—ã: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç ‚Üí –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (2‚Äì3 –ø—Ä–∏–º–µ—Ä–∞)\n",
        "indices = [0]\n",
        "if len(results) > 1:\n",
        "    indices.append(1)\n",
        "if len(results) > 3:\n",
        "    indices.append(len(results) // 2)\n",
        "for idx in indices:\n",
        "    if idx >= len(sample_texts):\n",
        "        break\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä {idx+1}\")\n",
        "    print(\"-\" * 40)\n",
        "    txt = sample_texts[idx]\n",
        "    print(\"–í—Ö–æ–¥ (—Å–æ–∫—Ä–∞—â—ë–Ω–Ω–æ):\", txt[:300] + \"...\" if len(txt) > 300 else txt)\n",
        "    print()\n",
        "    r = results[idx]\n",
        "    if \"raw\" in r:\n",
        "        print(\"–í—ã–≤–æ–¥ (raw):\", r[\"raw\"][:400] + \"...\" if len(r[\"raw\"]) > 400 else r[\"raw\"])\n",
        "    else:\n",
        "        print(\"–ò–∑–≤–ª–µ—á–µ–Ω–æ:\")\n",
        "        for k in entity_keys:\n",
        "            vals = r.get(k, [])\n",
        "            if vals:\n",
        "                print(f\"  {k}: {vals}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "print(\"–ò—Ç–æ–≥–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"–ú–æ–¥–µ–ª—å: {model_id}\")\n",
        "print(f\"–î–∏–∞–ª–æ–≥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {N_RUN}\")\n",
        "print(f\"–í—Ä–µ–º—è: {elapsed:.1f} —Å–µ–∫\")\n",
        "print(f\"–°–∫–æ—Ä–æ—Å—Ç—å: {N_RUN/elapsed:.3f} –¥–∏–∞–ª–æ–≥–æ–≤/—Å–µ–∫ (~{elapsed/N_RUN:.1f} —Å–µ–∫/–¥–∏–∞–ª–æ–≥)\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"VRAM –ø–∏–∫: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 –ß—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è\n",
        "\n",
        "| –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å | –ó–∞—á–µ–º |\n",
        "|---------------|-------|\n",
        "| –î–æ–ª—è –≤–∞–ª–∏–¥–Ω–æ–≥–æ JSON | –ù–∏–∑–∫–∞—è ‚Üí –¥–æ–±–∞–≤–∏—Ç—å –≤ –ø—Ä–æ–º–ø—Ç ¬´—Ç–æ–ª—å–∫–æ JSON, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ¬ª |\n",
        "| PERSON/ORG/LOC —á–∞—â–µ –ø—É—Å—Ç—ã–µ | –î–∏–∞–ª–æ–≥–∏ –º–æ–≥—É—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —è–≤–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π; –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä |\n",
        "| –ú–æ–¥–µ–ª—å –ø–∏—à–µ—Ç –ø–æ—è—Å–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ JSON | –Ø–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å: ¬´Output ONLY valid JSON, no extra text¬ª |\n",
        "| –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ | TinyLlama –±—ã—Å—Ç—Ä–µ–µ; batch-–∑–∞–ø—Ä–æ—Å—ã (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ) |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ï—Å–ª–∏ –º–Ω–æ–≥–æ raw ‚Äî –ø–æ–ø—Ä–æ–±—É–π –¥–æ–±–∞–≤–∏—Ç—å –≤ IE_PROMPT: \"Output ONLY valid JSON, no other text.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "\n",
        "–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º ‚Äî –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–∞–º—è—Ç—å (`del model`, `torch.cuda.empty_cache()`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ TinyLlama vs Mistral (–∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ –æ—á–µ—Ä–µ–¥–∏, –Ω–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)\n",
        "# 1) TinyLlama ~2GB VRAM, –±—ã—Å—Ç—Ä–µ–µ\n",
        "# 2) Mistral 4-bit ~6GB VRAM, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ\n",
        "# –ó–∞–º–µ—Ä—å –≤—Ä–µ–º—è –∏ VRAM –¥–ª—è –∫–∞–∂–¥–æ–π."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç ¬´–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω–µ—Ä¬ª\n",
        "\n",
        "–î–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –≤ —Å—Ç–∏–ª–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –¥–∏–∑–∞–π–Ω–µ—Ä–∞ ‚Äî —Å–º. `prompt_cognitive_designer.md` –∏–ª–∏ `../promt.md`. –î–æ–±–∞–≤—å –≤ –Ω–∞—á–∞–ª–æ –ø—Ä–æ–º–ø—Ç–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}